{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e49e8cf1-2783-43ae-b120-9a9a7902ed6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08416b20-41de-4137-936a-911b1ee20925",
   "metadata": {},
   "source": [
    "#### Configuracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67f171da-4bd6-4913-928c-7eb6f4c1063f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "image_name = sagemaker.image_uris.retrieve(\"forecasting-deepar\", region)\n",
    "role='LabRole'\n",
    "s3_bucket = 'mia-electiva3-dollar-predictor'\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, 'deepar-model')\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, 'deepar-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33809ccd-8799-4858-a6a4-763b97526886",
   "metadata": {},
   "source": [
    "### Cargar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8370dc7b-ce69-45e3-bbcf-5e3c0c3ee553",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cargar el dataset de precios por hora del dolar\n",
    "df = pd.read_csv('dollar_15min_price.csv', parse_dates=['fecha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "445388b0-ad77-4da6-8c25-08f411efc059",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha</th>\n",
       "      <th>precio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-17 13:15:00</td>\n",
       "      <td>4942.824392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-17 13:30:00</td>\n",
       "      <td>4936.291246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-17 13:45:00</td>\n",
       "      <td>4931.374835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-17 14:00:00</td>\n",
       "      <td>4931.280810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-17 14:15:00</td>\n",
       "      <td>4932.116995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                fecha       precio\n",
       "0 2023-02-17 13:15:00  4942.824392\n",
       "1 2023-02-17 13:30:00  4936.291246\n",
       "2 2023-02-17 13:45:00  4931.374835\n",
       "3 2023-02-17 14:00:00  4931.280810\n",
       "4 2023-02-17 14:15:00  4932.116995"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f335b96-d2ef-4d2a-b3d7-27b50fae5f99",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Entrenamiento - DeepAR\n",
    "El modelo por defecto de SageMaker para series de tiempo es DeepAR, y teniendo en cuenta los malos resultados obtenidos con otros modelos, se va a realizar el ejercicio de entrenamiento y despligue con este modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c4fc4df6-86a5-4b28-a130-4a0bd67443dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dividir el dataset en datos de entrenamiento y test\n",
    "train_size = math.ceil(len(df) * 0.7)\n",
    "train_data = df[:train_size]\n",
    "test_data = df[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb2ad8-9b54-46dd-915a-42a77ad520ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Convierto el dataset al formato json que acepta DeepAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c738b416-fe4e-44da-91ab-386c42cb994e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "freq = '15min' # La serie tiene registros cada 15 min\n",
    "prediction_length = 1 # se predice para el siguiete periodo\n",
    "context_length = 12 # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "47ac1302-41ee-4aed-8a6c-28e9142af396",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15963/1049242397.py:1: FutureWarning: The 'freq' argument in Timestamp is deprecated and will be removed in a future version.\n",
      "  start_dataset = pd.Timestamp(\"2023-02-17 13:15:00\", freq=freq)\n",
      "/tmp/ipykernel_15963/1049242397.py:2: FutureWarning: The 'freq' argument in Timestamp is deprecated and will be removed in a future version.\n",
      "  end_training = pd.Timestamp(\"2023-03-22 17:30:00\", freq=freq)\n"
     ]
    }
   ],
   "source": [
    "start_dataset = pd.Timestamp(\"2023-02-17 13:15:00\", freq=freq)\n",
    "end_training = pd.Timestamp(\"2023-03-22 17:30:00\", freq=freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0a1d62b4-e4df-47d6-8815-364428832be0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_daily_groups_train_data = train_data.groupby(pd.Grouper(key='fecha', freq='D'))\n",
    "df_daily_groups_test_data = test_data.groupby(pd.Grouper(key='fecha', freq='D'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e74e030c-19c4-412c-ab57-31c1ad4fbf2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': '2023-02-17 13:15:00', 'target': [4942.8243923076925, 4936.291245535715, 4931.37483539823, 4931.280809999999, 4932.116994776119, 4934.248586627908, 4935.192755, 4934.541332307693, 4931.933092647059, 4929.72935, 4926.492103278689, 4924.257728571428, 4923.086633333333, 4922.34411724138, 4921.625366666667, 4921.032043902439]}, {'start': '2023-02-20 13:15:00', 'target': [4891.17114375, 4896.2127, 4902.821188709677, 4908.80661875, 4910.851070588235, 4912.062606666667, 4912.2477, 4912.29805, 4912.352779999999, 4913.6919, 4913.7546, 4913.925657142858, 4914.092010526316, 4914.341209090909, 4914.4217, 4914.4267, 4914.4365, 4914.4387]}, {'start': '2023-02-21 13:00:00', 'target': [4920.99255, 4918.223595789474, 4923.14296, 4926.560226582279, 4929.801058108108, 4931.065254761905, 4931.688609459459, 4932.567686086957, 4934.754388709677, 4936.6460875, 4938.0391, 4939.871590769231, 4941.231171212121, 4943.047677777778, 4944.349084444444, 4945.869313793104, 4947.432927884615, 4948.429970422535, 4949.326057894737, 4949.894897590361]}, {'start': '2023-02-22 13:30:00', 'target': [4945.541823664122, 4942.173782, 4939.433314159292, 4938.324397014926, 4935.246913095239, 4932.934271666667, 4932.26645632184, 4931.868415217391, 4931.02291875, 4930.569701769911, 4930.114528235294, 4929.684514814815, 4929.4491419354845, 4929.280051376148, 4928.707039759036, 4927.934338181818, 4926.4463554054055, 4925.336481818182, 4924.7666]}, {'start': '2023-02-23 13:00:00', 'target': [4898.887717142858, 4880.073990769231, 4878.584703947368, 4876.883665517242, 4870.479862195121, 4864.93673508772, 4862.082446875, 4858.820303225806, 4856.141917647059, 4855.042235483871, 4854.29222, 4854.290451315789, 4853.9403, 4854.125940540541, 4854.382678125, 4854.3988500000005, 4854.350646666667, 4854.067597142857, 4853.845293333333, 4853.945977777777, 4853.9941]}]\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(group.iloc[0]['fecha']),\n",
    "        \"target\": group['precio'].values.tolist(),\n",
    "    }\n",
    "    for name, group in df_daily_groups_train_data if len(group) > 0\n",
    "]\n",
    "print(training_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "92f6a7a6-c8cb-4c1a-b07e-f7d1f286ac6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': '2023-03-30 13:00:00', 'target': [4596.001227777778, 4590.100217808219, 4588.54894074074, 4590.902668852459, 4598.702035365854, 4603.595745569621, 4609.294361764705, 4615.057370967742, 4619.613381481481, 4621.8191109375, 4622.793826530612, 4623.508959016393, 4624.295570526316, 4624.431183098592, 4624.452813043478, 4624.570473913043, 4624.841015384615, 4625.167286842106, 4625.739255, 4626.457698113208, 4626.919]}, {'start': '2023-03-31 13:00:00', 'target': [4621.521809375, 4631.085134883721, 4638.0944, 4641.4163475000005, 4642.24272962963, 4643.2688875, 4644.150120454546, 4645.2608635593215, 4647.431914285715, 4647.8581125, 4646.368723611111, 4645.338863492064, 4644.270390384616, 4643.658, 4643.436547058824, 4643.3061638888885, 4643.8560959459455, 4644.972011111111, 4645.480949999999, 4645.946115000001, 4646.3534]}, {'start': '2023-04-03 13:00:00', 'target': [4620.9185, 4614.185533333333, 4609.4488403225805, 4606.998575, 4605.45095, 4604.862125000001, 4604.704816393442, 4604.6855162162165, 4604.8726354166665, 4605.1378208333335, 4605.1917435483865, 4605.128335483871, 4604.239136538462, 4603.513870370371, 4603.203295, 4603.038576923077, 4602.928711111112, 4602.80313030303, 4602.763713636364, 4602.7269371428565]}, {'start': '2023-04-04 13:00:00', 'target': [4595.252095, 4593.923822413793, 4590.426159210526, 4590.583518367347, 4590.17887375, 4588.414323188405, 4587.703315000001, 4586.760543589744, 4586.146057407407, 4585.809606382979, 4586.469256470588, 4587.0755891891895, 4587.32014390244, 4587.693289189189, 4587.923519999999, 4587.897704761905, 4587.768128571429, 4587.6362625, 4587.3851982758615, 4587.196525714286]}, {'start': '2023-04-05 13:00:00', 'target': [4572.885475, 4568.115379999999, 4565.789935483871, 4565.378875, 4565.883435211267, 4566.944585416666, 4568.0758265625, 4570.21446, 4570.8674, 4571.347231914894, 4571.375505999999, 4571.2497079365085, 4571.053841304348, 4571.098029411764, 4571.158905555556, 4571.230763157895, 4571.232284313725, 4570.940455555556, 4570.781019354839, 4570.749058333333, 4570.7655]}]\n"
     ]
    }
   ],
   "source": [
    "testing_data = [\n",
    "    {\n",
    "        \"start\": str(group.iloc[0]['fecha']),\n",
    "        \"target\": group['precio'].values.tolist(),\n",
    "    }\n",
    "    for name, group in df_daily_groups_test_data if len(group) > 0\n",
    "]\n",
    "print(testing_data[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "765f8a28-e42b-419f-9e28-cdea3078591d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Guardo el archivo json para ser usado luego en el entranamiento\n",
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, \"wb\") as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6c77cb05-f71a-4dc4-8e78-8e79fc6ce711",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.46 ms, sys: 180 µs, total: 2.64 ms\n",
      "Wall time: 2.39 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "write_dicts_to_file(\"test.json\", testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c307d51f-f245-4e8b-a072-d8a750a25abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "def copy_to_s3(local_file, s3_path, override=True):\n",
    "    assert s3_path.startswith(\"s3://\")\n",
    "    split = s3_path.split(\"/\")\n",
    "    bucket = split[2]\n",
    "    path = \"/\".join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "\n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print(\n",
    "                \"File s3://{}/{} already exists.\\nSet override to upload anyway.\\n\".format(\n",
    "                    s3_bucket, s3_path\n",
    "                )\n",
    "            )\n",
    "            return\n",
    "        else:\n",
    "            print(\"Overwriting existing file\")\n",
    "    with open(local_file, \"rb\") as data:\n",
    "        print(\"Uploading file to {}\".format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "544ea6cc-2899-48d7-a9c4-0c2c477e6ab3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing file\n",
      "Uploading file to s3://mia-electiva3-dollar-predictor/deepar-model/data/train/train.json\n",
      "Overwriting existing file\n",
      "Uploading file to s3://mia-electiva3-dollar-predictor/deepar-model/data/test/test.json\n",
      "CPU times: user 28.2 ms, sys: 4.35 ms, total: 32.6 ms\n",
      "Wall time: 169 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train.json\", s3_data_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_data_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ef2fd-8029-4e2f-9b21-ddd77f1d42fe",
   "metadata": {},
   "source": [
    "##### Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2050e4e1-6c9c-47c4-ae18-e70bd9dd6d91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    base_job_name=\"dollar-predictor\",\n",
    "    output_path=s3_output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "537ea2c6-0bb2-4b37-a554-e5fbf2a17b27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "022914ee-ccdf-444c-8fbd-fe73595543f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2b8ba68f-3803-444d-bb2c-7448585c97a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\"train\": \"{}/train/\".format(s3_data_path), \"test\": \"{}/test/\".format(s3_data_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d2c31807-8268-48a1-9a7c-8a3e40e7eb22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: dollar-predictor-2023-04-08-01-02-47-418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-08 01:02:47 Starting - Starting the training job......\n",
      "2023-04-08 01:03:31 Starting - Preparing the instances for training.........\n",
      "2023-04-08 01:05:00 Downloading - Downloading input data...\n",
      "2023-04-08 01:05:25 Training - Downloading the training image.........\n",
      "2023-04-08 01:07:06 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:78: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:24 INFO 140448693614400] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:24 INFO 140448693614400] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '12', 'early_stopping_patience': '40', 'epochs': '400', 'learning_rate': '5E-4', 'mini_batch_size': '64', 'prediction_length': '1', 'time_freq': '15min'}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:24 INFO 140448693614400] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '12', 'epochs': '400', 'prediction_length': '1', 'time_freq': '15min'}\u001b[0m\n",
      "\u001b[34mProcess 7 is a worker.\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:24 INFO 140448693614400] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:24 INFO 140448693614400] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:24 INFO 140448693614400] random_seed is None\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] Training set statistics:\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] Real time series\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] number of time series: 22\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] number of observations: 437\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] mean target length: 19.863636363636363\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] min/mean/max target: 4676.78955078125/4821.960150886728/4949.89501953125\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] mean abs(target): 4821.960150886728\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] contains missing values: no\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] Small number of time series. Doing 30 passes over dataset with prob 0.9696969696969696 per epoch.\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] Test set statistics:\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] Real time series\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] number of time series: 10\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] number of observations: 187\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] mean target length: 18.7\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] min/mean/max target: 4565.37890625/4642.329378342246/4776.2490234375\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] mean abs(target): 4642.329378342246\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] contains missing values: no\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] #memory_usage::<batchbuffer> = 0.32958984375 mb\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] nvidia-smi: took 0.030 seconds to run.\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916045.0436978, \"EndTime\": 1680916045.0860448, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 41.17608070373535, \"count\": 1, \"min\": 41.17608070373535, \"max\": 41.17608070373535}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] #memory_usage::<model> = 5 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916045.0861368, \"EndTime\": 1680916045.14181, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 97.96810150146484, \"count\": 1, \"min\": 97.96810150146484, \"max\": 97.96810150146484}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] Epoch[0] Batch[0] avg_epoch_loss=7.013123\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=7.013123035430908\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] Epoch[0] Batch[5] avg_epoch_loss=7.474921\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=7.474920670191447\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] Epoch[0] Batch [5]#011Speed: 2711.13 samples/sec#011loss=7.474921\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] Epoch[0] Batch[10] avg_epoch_loss=7.115684\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=6.684599304199219\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] Epoch[0] Batch [10]#011Speed: 2068.43 samples/sec#011loss=6.684599\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916045.1418855, \"EndTime\": 1680916045.6897533, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 400.0, \"count\": 1, \"min\": 400, \"max\": 400}, \"update.time\": {\"sum\": 547.7554798126221, \"count\": 1, \"min\": 547.7554798126221, \"max\": 547.7554798126221}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1248.2184059425713 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] #progress_metric: host=algo-1, completed 0.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] #quality_metric: host=algo-1, epoch=0, train loss <loss>=7.115683685649525\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_13443e69-34f5-409a-9fa8-942583711300-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916045.68993, \"EndTime\": 1680916045.7065294, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 15.558004379272461, \"count\": 1, \"min\": 15.558004379272461, \"max\": 15.558004379272461}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] Epoch[1] Batch[0] avg_epoch_loss=6.679297\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:25 INFO 140448693614400] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=6.67929744720459\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] Epoch[1] Batch[5] avg_epoch_loss=6.726644\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=6.726643959681193\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] Epoch[1] Batch [5]#011Speed: 2285.76 samples/sec#011loss=6.726644\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916045.7066069, \"EndTime\": 1680916046.2381022, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 531.3615798950195, \"count\": 1, \"min\": 531.3615798950195, \"max\": 531.3615798950195}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1196.5889765390132 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] #quality_metric: host=algo-1, epoch=1, train loss <loss>=6.7366053581237795\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_5e0f67e8-e3e1-4aab-adc8-0f6723724da3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916046.2382135, \"EndTime\": 1680916046.249924, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 10.940790176391602, \"count\": 1, \"min\": 10.940790176391602, \"max\": 10.940790176391602}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] Epoch[2] Batch[0] avg_epoch_loss=6.073110\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=6.0731096267700195\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] Epoch[2] Batch[5] avg_epoch_loss=6.436487\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=6.436486721038818\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] Epoch[2] Batch [5]#011Speed: 2437.83 samples/sec#011loss=6.436487\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] Epoch[2] Batch[10] avg_epoch_loss=6.338214\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=6.2202863693237305\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] Epoch[2] Batch [10]#011Speed: 2247.41 samples/sec#011loss=6.220286\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] processed a total of 700 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916046.2500057, \"EndTime\": 1680916046.7219694, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 471.8959331512451, \"count\": 1, \"min\": 471.8959331512451, \"max\": 471.8959331512451}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1482.771093160142 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] #progress_metric: host=algo-1, completed 0.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] #quality_metric: host=algo-1, epoch=2, train loss <loss>=6.338213833895597\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_d7d40d33-2693-47f6-81fb-02fa84f32ba4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916046.722122, \"EndTime\": 1680916046.7373178, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 14.403581619262695, \"count\": 1, \"min\": 14.403581619262695, \"max\": 14.403581619262695}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] Epoch[3] Batch[0] avg_epoch_loss=6.601009\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:26 INFO 140448693614400] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=6.601009368896484\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] Epoch[3] Batch[5] avg_epoch_loss=6.269690\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=6.269689559936523\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] Epoch[3] Batch [5]#011Speed: 2348.16 samples/sec#011loss=6.269690\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] Epoch[3] Batch[10] avg_epoch_loss=6.107164\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=5.912133693695068\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] Epoch[3] Batch [10]#011Speed: 1948.54 samples/sec#011loss=5.912134\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] processed a total of 694 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916046.737497, \"EndTime\": 1680916047.2807345, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 543.0974960327148, \"count\": 1, \"min\": 543.0974960327148, \"max\": 543.0974960327148}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1277.3746994544872 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] #quality_metric: host=algo-1, epoch=3, train loss <loss>=6.107164166190407\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_e475fad9-5963-463b-be6f-c3e415d5fb35-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916047.280825, \"EndTime\": 1680916047.2962775, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 14.897823333740234, \"count\": 1, \"min\": 14.897823333740234, \"max\": 14.897823333740234}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] Epoch[4] Batch[0] avg_epoch_loss=6.269961\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=6.269960880279541\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] Epoch[4] Batch[5] avg_epoch_loss=5.560807\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=5.560807228088379\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] Epoch[4] Batch [5]#011Speed: 2988.21 samples/sec#011loss=5.560807\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] Epoch[4] Batch[10] avg_epoch_loss=5.433247\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=5.2801745414733885\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] Epoch[4] Batch [10]#011Speed: 2528.79 samples/sec#011loss=5.280175\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] processed a total of 702 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916047.2963586, \"EndTime\": 1680916047.7607079, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 464.28799629211426, \"count\": 1, \"min\": 464.28799629211426, \"max\": 464.28799629211426}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1511.57341246108 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] #progress_metric: host=algo-1, completed 1.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] #quality_metric: host=algo-1, epoch=4, train loss <loss>=5.433246915990656\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:27 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_c58e06a0-6550-482b-860f-447a2026401a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916047.760799, \"EndTime\": 1680916047.7757885, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 14.460086822509766, \"count\": 1, \"min\": 14.460086822509766, \"max\": 14.460086822509766}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] Epoch[5] Batch[0] avg_epoch_loss=5.127965\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=5.127965450286865\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] Epoch[5] Batch[5] avg_epoch_loss=5.280784\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=5.280784447987874\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] Epoch[5] Batch [5]#011Speed: 3111.16 samples/sec#011loss=5.280784\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] Epoch[5] Batch[10] avg_epoch_loss=5.218400\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=5.143538951873779\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] Epoch[5] Batch [10]#011Speed: 2745.61 samples/sec#011loss=5.143539\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916047.7758658, \"EndTime\": 1680916048.225658, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 449.7349262237549, \"count\": 1, \"min\": 449.7349262237549, \"max\": 449.7349262237549}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1478.2310910518968 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] #quality_metric: host=algo-1, epoch=5, train loss <loss>=5.218400131572377\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_5ae1a77e-3bc1-4564-9ec3-ef71ecb2482f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916048.2257488, \"EndTime\": 1680916048.241224, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 14.940023422241211, \"count\": 1, \"min\": 14.940023422241211, \"max\": 14.940023422241211}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] Epoch[6] Batch[0] avg_epoch_loss=4.406919\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=4.406919002532959\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] Epoch[6] Batch[5] avg_epoch_loss=4.666319\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=4.6663187344868975\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] Epoch[6] Batch [5]#011Speed: 2887.14 samples/sec#011loss=4.666319\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] Epoch[6] Batch[10] avg_epoch_loss=4.693708\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=4.726574230194092\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] Epoch[6] Batch [10]#011Speed: 2422.69 samples/sec#011loss=4.726574\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] processed a total of 690 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916048.2413156, \"EndTime\": 1680916048.7172399, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 475.8453369140625, \"count\": 1, \"min\": 475.8453369140625, \"max\": 475.8453369140625}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1449.5846994863966 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] #progress_metric: host=algo-1, completed 1.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] #quality_metric: host=algo-1, epoch=6, train loss <loss>=4.693707596171986\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_8ed8a8a0-ec77-4939-bfad-7408406d163d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916048.717347, \"EndTime\": 1680916048.7281973, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 10.213375091552734, \"count\": 1, \"min\": 10.213375091552734, \"max\": 10.213375091552734}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] Epoch[7] Batch[0] avg_epoch_loss=4.906437\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:28 INFO 140448693614400] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=4.906436920166016\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] Epoch[7] Batch[5] avg_epoch_loss=4.856221\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=4.856220960617065\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] Epoch[7] Batch [5]#011Speed: 2397.58 samples/sec#011loss=4.856221\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] Epoch[7] Batch[10] avg_epoch_loss=4.865206\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=4.87598762512207\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] Epoch[7] Batch [10]#011Speed: 2404.75 samples/sec#011loss=4.875988\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916048.7282915, \"EndTime\": 1680916049.17959, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 451.22694969177246, \"count\": 1, \"min\": 451.22694969177246, \"max\": 451.22694969177246}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1455.6208830419705 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] #quality_metric: host=algo-1, epoch=7, train loss <loss>=4.865205808119341\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] Epoch[8] Batch[0] avg_epoch_loss=4.771580\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=4.771579742431641\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] Epoch[8] Batch[5] avg_epoch_loss=4.506485\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=4.506485144297282\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] Epoch[8] Batch [5]#011Speed: 2790.74 samples/sec#011loss=4.506485\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916049.1796792, \"EndTime\": 1680916049.5638804, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 383.64601135253906, \"count\": 1, \"min\": 383.64601135253906, \"max\": 383.64601135253906}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1646.5926615192532 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] #progress_metric: host=algo-1, completed 2.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] #quality_metric: host=algo-1, epoch=8, train loss <loss>=4.578943729400635\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_cb748c09-6dbc-4b0e-9f60-49340ac1f693-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916049.5640128, \"EndTime\": 1680916049.5791001, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 14.494657516479492, \"count\": 1, \"min\": 14.494657516479492, \"max\": 14.494657516479492}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] Epoch[9] Batch[0] avg_epoch_loss=4.914366\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=4.914365768432617\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] Epoch[9] Batch[5] avg_epoch_loss=4.698569\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=4.6985689004262285\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:29 INFO 140448693614400] Epoch[9] Batch [5]#011Speed: 3059.67 samples/sec#011loss=4.698569\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] Epoch[9] Batch[10] avg_epoch_loss=4.668973\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=4.633458042144776\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] Epoch[9] Batch [10]#011Speed: 2556.75 samples/sec#011loss=4.633458\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916049.5791805, \"EndTime\": 1680916050.0409355, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 461.6892337799072, \"count\": 1, \"min\": 461.6892337799072, \"max\": 461.6892337799072}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1444.1629876036172 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] #quality_metric: host=algo-1, epoch=9, train loss <loss>=4.668973055752841\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] Epoch[10] Batch[0] avg_epoch_loss=4.374188\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=4.37418794631958\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] Epoch[10] Batch[5] avg_epoch_loss=4.587124\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=4.587123950322469\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] Epoch[10] Batch [5]#011Speed: 2808.54 samples/sec#011loss=4.587124\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] Epoch[10] Batch[10] avg_epoch_loss=4.510235\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=4.417967796325684\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] Epoch[10] Batch [10]#011Speed: 2477.42 samples/sec#011loss=4.417968\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] processed a total of 724 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916050.0410614, \"EndTime\": 1680916050.4855292, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 443.7754154205322, \"count\": 1, \"min\": 443.7754154205322, \"max\": 443.7754154205322}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1630.9677907240293 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] #progress_metric: host=algo-1, completed 2.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] #quality_metric: host=algo-1, epoch=10, train loss <loss>=4.4545237223307295\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_f9e0180c-bf80-44e4-bdad-2faf03403b3e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916050.4856143, \"EndTime\": 1680916050.4988925, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.609720230102539, \"count\": 1, \"min\": 12.609720230102539, \"max\": 12.609720230102539}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] Epoch[11] Batch[0] avg_epoch_loss=4.271036\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=4.271035671234131\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] Epoch[11] Batch[5] avg_epoch_loss=4.375059\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=4.3750585714976\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] Epoch[11] Batch [5]#011Speed: 2654.05 samples/sec#011loss=4.375059\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] Epoch[11] Batch[10] avg_epoch_loss=4.556634\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=4.774523544311523\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] Epoch[11] Batch [10]#011Speed: 2491.69 samples/sec#011loss=4.774524\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916050.4989798, \"EndTime\": 1680916050.989742, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 490.69666862487793, \"count\": 1, \"min\": 490.69666862487793, \"max\": 490.69666862487793}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1346.7156097352458 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] #quality_metric: host=algo-1, epoch=11, train loss <loss>=4.556633559140292\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:30 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] Epoch[12] Batch[0] avg_epoch_loss=4.841438\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=4.841437816619873\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] Epoch[12] Batch[5] avg_epoch_loss=4.613418\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=4.613417625427246\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] Epoch[12] Batch [5]#011Speed: 3138.05 samples/sec#011loss=4.613418\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] Epoch[12] Batch[10] avg_epoch_loss=4.635251\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=4.66145076751709\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] Epoch[12] Batch [10]#011Speed: 2539.60 samples/sec#011loss=4.661451\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] processed a total of 687 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916050.989831, \"EndTime\": 1680916051.4442246, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 453.782320022583, \"count\": 1, \"min\": 453.782320022583, \"max\": 453.782320022583}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1513.3009058839655 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] #progress_metric: host=algo-1, completed 3.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] #quality_metric: host=algo-1, epoch=12, train loss <loss>=4.635250871831721\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] Epoch[13] Batch[0] avg_epoch_loss=4.378051\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=4.378050804138184\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] Epoch[13] Batch[5] avg_epoch_loss=4.381538\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=4.381537834803264\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] Epoch[13] Batch [5]#011Speed: 2425.41 samples/sec#011loss=4.381538\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] Epoch[13] Batch[10] avg_epoch_loss=4.386266\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=4.391939163208008\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] Epoch[13] Batch [10]#011Speed: 2303.15 samples/sec#011loss=4.391939\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] processed a total of 713 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916051.4443746, \"EndTime\": 1680916051.9681907, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 523.1211185455322, \"count\": 1, \"min\": 523.1211185455322, \"max\": 523.1211185455322}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1362.5514177094306 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] #quality_metric: host=algo-1, epoch=13, train loss <loss>=4.4962157011032104\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:31 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] Epoch[14] Batch[0] avg_epoch_loss=4.593256\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=4.593255996704102\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] Epoch[14] Batch[5] avg_epoch_loss=4.357194\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=4.357193946838379\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] Epoch[14] Batch [5]#011Speed: 3100.96 samples/sec#011loss=4.357194\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] Epoch[14] Batch[10] avg_epoch_loss=4.345708\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=4.331925678253174\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] Epoch[14] Batch [10]#011Speed: 2753.67 samples/sec#011loss=4.331926\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916051.9682963, \"EndTime\": 1680916052.421789, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 452.75211334228516, \"count\": 1, \"min\": 452.75211334228516, \"max\": 452.75211334228516}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1472.7291797286064 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] #progress_metric: host=algo-1, completed 3.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] #quality_metric: host=algo-1, epoch=14, train loss <loss>=4.34570837020874\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_920f32cf-1ee5-4c11-b760-290e0828738c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916052.4218972, \"EndTime\": 1680916052.437099, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 14.687538146972656, \"count\": 1, \"min\": 14.687538146972656, \"max\": 14.687538146972656}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] Epoch[15] Batch[0] avg_epoch_loss=4.343841\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=4.343840599060059\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] Epoch[15] Batch[5] avg_epoch_loss=4.335615\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=4.335615038871765\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] Epoch[15] Batch [5]#011Speed: 2991.69 samples/sec#011loss=4.335615\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] Epoch[15] Batch[10] avg_epoch_loss=4.442460\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=4.5706733703613285\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] Epoch[15] Batch [10]#011Speed: 2636.31 samples/sec#011loss=4.570673\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916052.4371727, \"EndTime\": 1680916052.8969853, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 459.7494602203369, \"count\": 1, \"min\": 459.7494602203369, \"max\": 459.7494602203369}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1483.0351784117274 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] #quality_metric: host=algo-1, epoch=15, train loss <loss>=4.442459735003385\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:32 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] Epoch[16] Batch[0] avg_epoch_loss=4.440224\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=4.440223693847656\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] Epoch[16] Batch[5] avg_epoch_loss=4.329803\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=4.329802910486857\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] Epoch[16] Batch [5]#011Speed: 2847.54 samples/sec#011loss=4.329803\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916052.8970618, \"EndTime\": 1680916053.28555, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 387.56752014160156, \"count\": 1, \"min\": 387.56752014160156, \"max\": 387.56752014160156}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1634.7609084420587 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] #progress_metric: host=algo-1, completed 4.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] #quality_metric: host=algo-1, epoch=16, train loss <loss>=4.275266647338867\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_47f7b4af-3eab-439b-bd29-a07d916a732d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916053.2857554, \"EndTime\": 1680916053.295662, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.213685989379883, \"count\": 1, \"min\": 9.213685989379883, \"max\": 9.213685989379883}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] Epoch[17] Batch[0] avg_epoch_loss=4.047423\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=4.047422885894775\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] Epoch[17] Batch[5] avg_epoch_loss=4.115543\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=4.115542729695638\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] Epoch[17] Batch [5]#011Speed: 3123.90 samples/sec#011loss=4.115543\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] Epoch[17] Batch[10] avg_epoch_loss=4.358753\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=4.65060510635376\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] Epoch[17] Batch [10]#011Speed: 2600.67 samples/sec#011loss=4.650605\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916053.2957957, \"EndTime\": 1680916053.708915, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 413.0439758300781, \"count\": 1, \"min\": 413.0439758300781, \"max\": 413.0439758300781}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1597.3792332207533 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] #quality_metric: host=algo-1, epoch=17, train loss <loss>=4.358752900903875\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] Epoch[18] Batch[0] avg_epoch_loss=4.207382\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=4.2073822021484375\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] Epoch[18] Batch[5] avg_epoch_loss=4.163297\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=4.163297136624654\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:33 INFO 140448693614400] Epoch[18] Batch [5]#011Speed: 3148.88 samples/sec#011loss=4.163297\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] Epoch[18] Batch[10] avg_epoch_loss=4.272247\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=4.402987861633301\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] Epoch[18] Batch [10]#011Speed: 2626.47 samples/sec#011loss=4.402988\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916053.7090046, \"EndTime\": 1680916054.121124, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 411.56578063964844, \"count\": 1, \"min\": 411.56578063964844, \"max\": 411.56578063964844}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1629.7925820230341 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] #progress_metric: host=algo-1, completed 4.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] #quality_metric: host=algo-1, epoch=18, train loss <loss>=4.272247466174039\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_490930c9-db10-41b5-94a1-bc500fd9eb86-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916054.1212268, \"EndTime\": 1680916054.1361253, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 14.265775680541992, \"count\": 1, \"min\": 14.265775680541992, \"max\": 14.265775680541992}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] Epoch[19] Batch[0] avg_epoch_loss=3.885591\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=3.8855912685394287\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] Epoch[19] Batch[5] avg_epoch_loss=4.358549\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=4.358549237251282\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] Epoch[19] Batch [5]#011Speed: 3071.60 samples/sec#011loss=4.358549\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] Epoch[19] Batch[10] avg_epoch_loss=4.435160\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=4.527093982696533\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] Epoch[19] Batch [10]#011Speed: 3013.45 samples/sec#011loss=4.527094\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916054.1361983, \"EndTime\": 1680916054.5510678, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 414.8106575012207, \"count\": 1, \"min\": 414.8106575012207, \"max\": 414.8106575012207}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1547.235205630106 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] #quality_metric: host=algo-1, epoch=19, train loss <loss>=4.435160485180941\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] Epoch[20] Batch[0] avg_epoch_loss=4.541786\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=4.541785717010498\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] Epoch[20] Batch[5] avg_epoch_loss=4.438192\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=4.438191731770833\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:34 INFO 140448693614400] Epoch[20] Batch [5]#011Speed: 2923.87 samples/sec#011loss=4.438192\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] Epoch[20] Batch[10] avg_epoch_loss=4.440137\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=4.442471218109131\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] Epoch[20] Batch [10]#011Speed: 2598.60 samples/sec#011loss=4.442471\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] processed a total of 705 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916054.5511541, \"EndTime\": 1680916055.0353062, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 483.6733341217041, \"count\": 1, \"min\": 483.6733341217041, \"max\": 483.6733341217041}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1457.19022470022 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] #progress_metric: host=algo-1, completed 5.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=20, train loss <loss>=4.5810054540634155\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] Epoch[21] Batch[0] avg_epoch_loss=4.597379\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=4.597378730773926\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] Epoch[21] Batch[5] avg_epoch_loss=4.385353\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=4.385353247324626\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] Epoch[21] Batch [5]#011Speed: 3142.88 samples/sec#011loss=4.385353\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] Epoch[21] Batch[10] avg_epoch_loss=4.360772\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=4.331275177001953\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] Epoch[21] Batch [10]#011Speed: 2975.58 samples/sec#011loss=4.331275\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916055.0354, \"EndTime\": 1680916055.4802527, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 444.3516731262207, \"count\": 1, \"min\": 444.3516731262207, \"max\": 444.3516731262207}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1473.6512718183087 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=21, train loss <loss>=4.360772306268865\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] Epoch[22] Batch[0] avg_epoch_loss=4.135480\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=4.1354804039001465\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] Epoch[22] Batch[5] avg_epoch_loss=4.270757\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=4.270756642023723\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] Epoch[22] Batch [5]#011Speed: 2914.68 samples/sec#011loss=4.270757\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] Epoch[22] Batch[10] avg_epoch_loss=4.253844\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=4.233547782897949\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] Epoch[22] Batch [10]#011Speed: 2699.66 samples/sec#011loss=4.233548\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916055.480336, \"EndTime\": 1680916055.9398112, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 458.9989185333252, \"count\": 1, \"min\": 458.9989185333252, \"max\": 458.9989185333252}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1480.7588915568824 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] #progress_metric: host=algo-1, completed 5.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=22, train loss <loss>=4.25384352423928\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:35 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_c9e2ea85-20c3-41e3-8055-0abdc03b9c32-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916055.9399905, \"EndTime\": 1680916055.9504037, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.578704833984375, \"count\": 1, \"min\": 9.578704833984375, \"max\": 9.578704833984375}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] Epoch[23] Batch[0] avg_epoch_loss=4.089687\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=4.089687347412109\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] Epoch[23] Batch[5] avg_epoch_loss=4.215546\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=4.215546131134033\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] Epoch[23] Batch [5]#011Speed: 2921.44 samples/sec#011loss=4.215546\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] Epoch[23] Batch[10] avg_epoch_loss=4.232486\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=4.252813816070557\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] Epoch[23] Batch [10]#011Speed: 2726.27 samples/sec#011loss=4.252814\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] processed a total of 691 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916055.9505358, \"EndTime\": 1680916056.354373, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 403.77140045166016, \"count\": 1, \"min\": 403.77140045166016, \"max\": 403.77140045166016}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1710.590686688237 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] #quality_metric: host=algo-1, epoch=23, train loss <loss>=4.232485987923362\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_b3f81dca-28bb-4876-b15f-a8fae4c2b454-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916056.3545153, \"EndTime\": 1680916056.365867, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 10.564088821411133, \"count\": 1, \"min\": 10.564088821411133, \"max\": 10.564088821411133}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] Epoch[24] Batch[0] avg_epoch_loss=4.069638\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=4.069638252258301\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] Epoch[24] Batch[5] avg_epoch_loss=4.160833\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=4.16083304087321\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] Epoch[24] Batch [5]#011Speed: 3129.03 samples/sec#011loss=4.160833\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] Epoch[24] Batch[10] avg_epoch_loss=4.181740\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=4.206829404830932\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] Epoch[24] Batch [10]#011Speed: 2495.56 samples/sec#011loss=4.206829\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916056.3659527, \"EndTime\": 1680916056.7760155, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 409.99913215637207, \"count\": 1, \"min\": 409.99913215637207, \"max\": 409.99913215637207}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1640.9287110697728 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] #progress_metric: host=algo-1, completed 6.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] #quality_metric: host=algo-1, epoch=24, train loss <loss>=4.181740479035811\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:36 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_348b5fb4-2f9f-4ddc-b596-61e342477415-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916056.776111, \"EndTime\": 1680916056.7912538, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 14.612913131713867, \"count\": 1, \"min\": 14.612913131713867, \"max\": 14.612913131713867}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] Epoch[25] Batch[0] avg_epoch_loss=3.950205\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=3.9502053260803223\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] Epoch[25] Batch[5] avg_epoch_loss=4.279231\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=4.279231071472168\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] Epoch[25] Batch [5]#011Speed: 2812.81 samples/sec#011loss=4.279231\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916056.791339, \"EndTime\": 1680916057.2368343, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 445.4360008239746, \"count\": 1, \"min\": 445.4360008239746, \"max\": 445.4360008239746}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1425.1965262954554 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] #quality_metric: host=algo-1, epoch=25, train loss <loss>=4.2966883182525635\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] Epoch[26] Batch[0] avg_epoch_loss=4.315138\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=4.315138339996338\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] Epoch[26] Batch[5] avg_epoch_loss=4.349227\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=4.34922726949056\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] Epoch[26] Batch [5]#011Speed: 2365.09 samples/sec#011loss=4.349227\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916057.2369153, \"EndTime\": 1680916057.6517959, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 414.25299644470215, \"count\": 1, \"min\": 414.25299644470215, \"max\": 414.25299644470215}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1544.42071735886 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] #progress_metric: host=algo-1, completed 6.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] #quality_metric: host=algo-1, epoch=26, train loss <loss>=4.3117859125137326\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] Epoch[27] Batch[0] avg_epoch_loss=4.057069\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=4.057069301605225\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] Epoch[27] Batch[5] avg_epoch_loss=4.142339\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=4.14233938852946\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:37 INFO 140448693614400] Epoch[27] Batch [5]#011Speed: 2917.71 samples/sec#011loss=4.142339\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] Epoch[27] Batch[10] avg_epoch_loss=4.173816\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=4.211587142944336\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] Epoch[27] Batch [10]#011Speed: 2638.85 samples/sec#011loss=4.211587\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] processed a total of 689 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916057.6518908, \"EndTime\": 1680916058.0772808, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 424.3643283843994, \"count\": 1, \"min\": 424.3643283843994, \"max\": 424.3643283843994}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1623.0641491800645 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] #quality_metric: host=algo-1, epoch=27, train loss <loss>=4.173815640536222\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_59e75e65-89af-45e6-b739-885f5bd7a0f2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916058.0773804, \"EndTime\": 1680916058.0924706, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 14.569282531738281, \"count\": 1, \"min\": 14.569282531738281, \"max\": 14.569282531738281}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] Epoch[28] Batch[0] avg_epoch_loss=4.307689\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=4.3076887130737305\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] Epoch[28] Batch[5] avg_epoch_loss=4.190857\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=4.190857013066609\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] Epoch[28] Batch [5]#011Speed: 2947.15 samples/sec#011loss=4.190857\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] Epoch[28] Batch[10] avg_epoch_loss=4.164338\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=4.1325146675109865\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] Epoch[28] Batch [10]#011Speed: 2497.19 samples/sec#011loss=4.132515\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] processed a total of 716 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916058.0925474, \"EndTime\": 1680916058.5808687, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 488.2533550262451, \"count\": 1, \"min\": 488.2533550262451, \"max\": 488.2533550262451}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1466.0530276698366 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] #progress_metric: host=algo-1, completed 7.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] #quality_metric: host=algo-1, epoch=28, train loss <loss>=4.311043540636699\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] Epoch[29] Batch[0] avg_epoch_loss=4.189887\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=4.189886569976807\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] Epoch[29] Batch[5] avg_epoch_loss=4.116963\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=4.116963267326355\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:38 INFO 140448693614400] Epoch[29] Batch [5]#011Speed: 3078.66 samples/sec#011loss=4.116963\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] Epoch[29] Batch[10] avg_epoch_loss=4.289364\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=4.4962457656860355\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] Epoch[29] Batch [10]#011Speed: 2998.64 samples/sec#011loss=4.496246\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916058.5809574, \"EndTime\": 1680916059.0019884, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 420.5048084259033, \"count\": 1, \"min\": 420.5048084259033, \"max\": 420.5048084259033}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1533.3507980114753 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] #quality_metric: host=algo-1, epoch=29, train loss <loss>=4.289364402944392\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] Epoch[30] Batch[0] avg_epoch_loss=3.977298\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=3.97729754447937\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] Epoch[30] Batch[5] avg_epoch_loss=4.064496\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=4.064495801925659\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] Epoch[30] Batch [5]#011Speed: 3140.96 samples/sec#011loss=4.064496\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] Epoch[30] Batch[10] avg_epoch_loss=4.032388\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=3.993857765197754\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] Epoch[30] Batch [10]#011Speed: 2489.34 samples/sec#011loss=3.993858\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916059.0020833, \"EndTime\": 1680916059.4097729, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 407.1063995361328, \"count\": 1, \"min\": 407.1063995361328, \"max\": 407.1063995361328}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1640.2118217459288 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] #progress_metric: host=algo-1, completed 7.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] #quality_metric: host=algo-1, epoch=30, train loss <loss>=4.032387603412975\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_52492d44-4ec6-4132-81c6-1d04b06c275d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916059.409885, \"EndTime\": 1680916059.4212556, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 10.790109634399414, \"count\": 1, \"min\": 10.790109634399414, \"max\": 10.790109634399414}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] Epoch[31] Batch[0] avg_epoch_loss=3.599766\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=3.5997655391693115\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] Epoch[31] Batch[5] avg_epoch_loss=4.008365\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=4.008364756902059\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] Epoch[31] Batch [5]#011Speed: 3129.01 samples/sec#011loss=4.008365\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] Epoch[31] Batch[10] avg_epoch_loss=4.005109\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=4.001201820373535\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] Epoch[31] Batch [10]#011Speed: 2412.50 samples/sec#011loss=4.001202\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] processed a total of 693 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916059.4213407, \"EndTime\": 1680916059.8327534, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 411.3442897796631, \"count\": 1, \"min\": 411.3442897796631, \"max\": 411.3442897796631}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1684.1978632988728 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] #quality_metric: host=algo-1, epoch=31, train loss <loss>=4.005108876661821\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:39 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_e8524878-d283-4a85-9d16-b3b26117d81f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916059.8328412, \"EndTime\": 1680916059.842823, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.344100952148438, \"count\": 1, \"min\": 9.344100952148438, \"max\": 9.344100952148438}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] Epoch[32] Batch[0] avg_epoch_loss=4.265121\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=4.265120983123779\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] Epoch[32] Batch[5] avg_epoch_loss=4.093501\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=4.093501091003418\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] Epoch[32] Batch [5]#011Speed: 2848.38 samples/sec#011loss=4.093501\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] Epoch[32] Batch[10] avg_epoch_loss=4.209500\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=4.348698091506958\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] Epoch[32] Batch [10]#011Speed: 2834.68 samples/sec#011loss=4.348698\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916059.8428986, \"EndTime\": 1680916060.2522707, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 409.2836380004883, \"count\": 1, \"min\": 409.2836380004883, \"max\": 409.2836380004883}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1616.7510365881708 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] #progress_metric: host=algo-1, completed 8.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] #quality_metric: host=algo-1, epoch=32, train loss <loss>=4.2094997275959365\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] Epoch[33] Batch[0] avg_epoch_loss=4.393686\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=4.393685817718506\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] Epoch[33] Batch[5] avg_epoch_loss=4.115415\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=4.115415294965108\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] Epoch[33] Batch [5]#011Speed: 2415.66 samples/sec#011loss=4.115415\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916060.2523847, \"EndTime\": 1680916060.6690457, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 415.9266948699951, \"count\": 1, \"min\": 415.9266948699951, \"max\": 415.9266948699951}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1482.9703398547565 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] #quality_metric: host=algo-1, epoch=33, train loss <loss>=4.203041005134582\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] Epoch[34] Batch[0] avg_epoch_loss=3.978673\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=3.978672504425049\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] Epoch[34] Batch[5] avg_epoch_loss=4.142203\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=4.1422029336293535\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:40 INFO 140448693614400] Epoch[34] Batch [5]#011Speed: 2957.02 samples/sec#011loss=4.142203\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] Epoch[34] Batch[10] avg_epoch_loss=4.116680\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=4.086053419113159\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] Epoch[34] Batch [10]#011Speed: 2560.40 samples/sec#011loss=4.086053\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916060.669132, \"EndTime\": 1680916061.0919912, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 422.2142696380615, \"count\": 1, \"min\": 422.2142696380615, \"max\": 422.2142696380615}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1586.25483183411 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] #progress_metric: host=algo-1, completed 8.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] #quality_metric: host=algo-1, epoch=34, train loss <loss>=4.116680427031084\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] Epoch[35] Batch[0] avg_epoch_loss=4.389537\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=4.389537334442139\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] Epoch[35] Batch[5] avg_epoch_loss=4.113717\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=4.113716562589009\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] Epoch[35] Batch [5]#011Speed: 2806.34 samples/sec#011loss=4.113717\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] Epoch[35] Batch[10] avg_epoch_loss=4.057561\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=3.9901734828948974\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] Epoch[35] Batch [10]#011Speed: 2720.45 samples/sec#011loss=3.990173\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916061.0920832, \"EndTime\": 1680916061.563327, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 470.6602096557617, \"count\": 1, \"min\": 470.6602096557617, \"max\": 470.6602096557617}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1418.8912048138468 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] #quality_metric: host=algo-1, epoch=35, train loss <loss>=4.057560617273504\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] Epoch[36] Batch[0] avg_epoch_loss=4.439734\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=4.439733982086182\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] Epoch[36] Batch[5] avg_epoch_loss=4.153964\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=4.153963963190715\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:41 INFO 140448693614400] Epoch[36] Batch [5]#011Speed: 2802.92 samples/sec#011loss=4.153964\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] Epoch[36] Batch[10] avg_epoch_loss=3.908408\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=3.6137415885925295\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] Epoch[36] Batch [10]#011Speed: 2908.80 samples/sec#011loss=3.613742\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916061.5634146, \"EndTime\": 1680916062.0208287, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 456.94518089294434, \"count\": 1, \"min\": 456.94518089294434, \"max\": 456.94518089294434}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1430.7691026980451 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] #progress_metric: host=algo-1, completed 9.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] #quality_metric: host=algo-1, epoch=36, train loss <loss>=3.9084083383733574\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_707b65d9-e3e6-48f3-a3ce-691d39356eeb-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916062.020935, \"EndTime\": 1680916062.0333972, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.844635009765625, \"count\": 1, \"min\": 11.844635009765625, \"max\": 11.844635009765625}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] Epoch[37] Batch[0] avg_epoch_loss=3.789406\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=3.7894062995910645\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] Epoch[37] Batch[5] avg_epoch_loss=4.218516\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=4.218515793482463\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] Epoch[37] Batch [5]#011Speed: 2900.01 samples/sec#011loss=4.218516\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] Epoch[37] Batch[10] avg_epoch_loss=4.158510\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=4.086502981185913\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] Epoch[37] Batch [10]#011Speed: 2414.39 samples/sec#011loss=4.086503\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916062.0334823, \"EndTime\": 1680916062.4545474, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 420.99761962890625, \"count\": 1, \"min\": 420.99761962890625, \"max\": 420.99761962890625}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1626.580945252958 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] #quality_metric: host=algo-1, epoch=37, train loss <loss>=4.158509969711304\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] Epoch[38] Batch[0] avg_epoch_loss=4.388449\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=4.388449192047119\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] Epoch[38] Batch[5] avg_epoch_loss=4.120918\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=4.120918234189351\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] Epoch[38] Batch [5]#011Speed: 2539.60 samples/sec#011loss=4.120918\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] Epoch[38] Batch[10] avg_epoch_loss=4.105849\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=4.0877649784088135\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] Epoch[38] Batch [10]#011Speed: 2513.23 samples/sec#011loss=4.087765\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916062.4546375, \"EndTime\": 1680916062.940746, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 485.5766296386719, \"count\": 1, \"min\": 485.5766296386719, \"max\": 485.5766296386719}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1416.500049332635 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] #progress_metric: host=algo-1, completed 9.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] #quality_metric: host=algo-1, epoch=38, train loss <loss>=4.105848572470925\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:42 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] Epoch[39] Batch[0] avg_epoch_loss=4.295070\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=4.295069694519043\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] Epoch[39] Batch[5] avg_epoch_loss=4.124186\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=4.124186356862386\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] Epoch[39] Batch [5]#011Speed: 2761.23 samples/sec#011loss=4.124186\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] Epoch[39] Batch[10] avg_epoch_loss=3.945914\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=3.731986379623413\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] Epoch[39] Batch [10]#011Speed: 2521.95 samples/sec#011loss=3.731986\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916062.940833, \"EndTime\": 1680916063.3929555, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 451.59006118774414, \"count\": 1, \"min\": 451.59006118774414, \"max\": 451.59006118774414}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1503.0737299438076 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] #quality_metric: host=algo-1, epoch=39, train loss <loss>=3.94591363993558\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] Epoch[40] Batch[0] avg_epoch_loss=4.141557\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=4.141557216644287\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] Epoch[40] Batch[5] avg_epoch_loss=3.970432\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=3.970431685447693\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] Epoch[40] Batch [5]#011Speed: 3040.32 samples/sec#011loss=3.970432\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] Epoch[40] Batch[10] avg_epoch_loss=4.055806\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=4.1582550525665285\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] Epoch[40] Batch [10]#011Speed: 2746.02 samples/sec#011loss=4.158255\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916063.3930626, \"EndTime\": 1680916063.8038356, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 410.20941734313965, \"count\": 1, \"min\": 410.20941734313965, \"max\": 410.20941734313965}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1627.7203694881775 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] #progress_metric: host=algo-1, completed 10.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] #quality_metric: host=algo-1, epoch=40, train loss <loss>=4.0558059432289815\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:43 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] Epoch[41] Batch[0] avg_epoch_loss=4.067992\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=4.067991733551025\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] Epoch[41] Batch[5] avg_epoch_loss=4.072796\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=4.072795987129211\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] Epoch[41] Batch [5]#011Speed: 3171.99 samples/sec#011loss=4.072796\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916063.8039742, \"EndTime\": 1680916064.2157378, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 411.2813472747803, \"count\": 1, \"min\": 411.2813472747803, \"max\": 411.2813472747803}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1540.9860660473412 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] #quality_metric: host=algo-1, epoch=41, train loss <loss>=4.054652404785156\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] Epoch[42] Batch[0] avg_epoch_loss=4.087625\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=4.087624549865723\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] Epoch[42] Batch[5] avg_epoch_loss=4.012660\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=4.0126597086588545\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] Epoch[42] Batch [5]#011Speed: 3115.78 samples/sec#011loss=4.012660\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] Epoch[42] Batch[10] avg_epoch_loss=4.057074\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=4.1103722095489506\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] Epoch[42] Batch [10]#011Speed: 2859.16 samples/sec#011loss=4.110372\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916064.215838, \"EndTime\": 1680916064.6573617, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 440.74082374572754, \"count\": 1, \"min\": 440.74082374572754, \"max\": 440.74082374572754}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1524.2215978040092 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] #progress_metric: host=algo-1, completed 10.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] #quality_metric: host=algo-1, epoch=42, train loss <loss>=4.057074481790716\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] Epoch[43] Batch[0] avg_epoch_loss=3.695737\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=3.69573712348938\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] Epoch[43] Batch[5] avg_epoch_loss=3.903641\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=3.9036405086517334\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:44 INFO 140448693614400] Epoch[43] Batch [5]#011Speed: 2676.54 samples/sec#011loss=3.903641\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] Epoch[43] Batch[10] avg_epoch_loss=3.709181\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=3.475829601287842\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] Epoch[43] Batch [10]#011Speed: 2385.61 samples/sec#011loss=3.475830\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916064.6574576, \"EndTime\": 1680916065.0870736, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 428.9684295654297, \"count\": 1, \"min\": 428.9684295654297, \"max\": 428.9684295654297}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1521.7685244090874 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] #quality_metric: host=algo-1, epoch=43, train loss <loss>=3.70918100530451\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_32f01a97-8137-4481-8527-91fb7cf0b604-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916065.087164, \"EndTime\": 1680916065.0978038, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 10.000944137573242, \"count\": 1, \"min\": 10.000944137573242, \"max\": 10.000944137573242}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] Epoch[44] Batch[0] avg_epoch_loss=4.135265\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=4.135265350341797\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] Epoch[44] Batch[5] avg_epoch_loss=3.991029\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=3.9910291830698648\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] Epoch[44] Batch [5]#011Speed: 2660.19 samples/sec#011loss=3.991029\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] Epoch[44] Batch[10] avg_epoch_loss=3.945371\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=3.890580081939697\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] Epoch[44] Batch [10]#011Speed: 2193.15 samples/sec#011loss=3.890580\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] processed a total of 699 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916065.0978906, \"EndTime\": 1680916065.5422094, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 444.2570209503174, \"count\": 1, \"min\": 444.2570209503174, \"max\": 444.2570209503174}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1572.9686395446906 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] #progress_metric: host=algo-1, completed 11.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] #quality_metric: host=algo-1, epoch=44, train loss <loss>=3.9453705007379707\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] Epoch[45] Batch[0] avg_epoch_loss=3.894957\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=3.8949568271636963\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] Epoch[45] Batch[5] avg_epoch_loss=3.842496\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=3.8424957195917764\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:45 INFO 140448693614400] Epoch[45] Batch [5]#011Speed: 2683.91 samples/sec#011loss=3.842496\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] Epoch[45] Batch[10] avg_epoch_loss=3.874954\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=3.9139048576354982\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] Epoch[45] Batch [10]#011Speed: 2603.94 samples/sec#011loss=3.913905\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916065.5422976, \"EndTime\": 1680916066.016694, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 473.9036560058594, \"count\": 1, \"min\": 473.9036560058594, \"max\": 473.9036560058594}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1451.2870667283592 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] #quality_metric: host=algo-1, epoch=45, train loss <loss>=3.874954418702559\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] Epoch[46] Batch[0] avg_epoch_loss=3.861371\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=3.8613712787628174\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] Epoch[46] Batch[5] avg_epoch_loss=4.054137\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=4.054137388865153\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] Epoch[46] Batch [5]#011Speed: 2889.78 samples/sec#011loss=4.054137\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] Epoch[46] Batch[10] avg_epoch_loss=4.075254\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=4.100593090057373\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] Epoch[46] Batch [10]#011Speed: 2272.19 samples/sec#011loss=4.100593\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916066.016783, \"EndTime\": 1680916066.4902253, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 472.8245735168457, \"count\": 1, \"min\": 472.8245735168457, \"max\": 472.8245735168457}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1412.3410291890038 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] #progress_metric: host=algo-1, completed 11.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] #quality_metric: host=algo-1, epoch=46, train loss <loss>=4.075253616679799\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] Epoch[47] Batch[0] avg_epoch_loss=4.431083\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=4.431082725524902\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] Epoch[47] Batch[5] avg_epoch_loss=4.019705\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=4.019704977671306\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] Epoch[47] Batch [5]#011Speed: 3106.36 samples/sec#011loss=4.019705\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] Epoch[47] Batch[10] avg_epoch_loss=3.973614\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=3.9183040618896485\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] Epoch[47] Batch [10]#011Speed: 2449.13 samples/sec#011loss=3.918304\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916066.4903295, \"EndTime\": 1680916066.9088862, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 417.9399013519287, \"count\": 1, \"min\": 417.9399013519287, \"max\": 417.9399013519287}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1628.9664662244163 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] #quality_metric: host=algo-1, epoch=47, train loss <loss>=3.973613652316007\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:46 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] Epoch[48] Batch[0] avg_epoch_loss=4.140292\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=4.140291690826416\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] Epoch[48] Batch[5] avg_epoch_loss=3.964465\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=3.9644649426142373\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] Epoch[48] Batch [5]#011Speed: 3082.88 samples/sec#011loss=3.964465\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] Epoch[48] Batch[10] avg_epoch_loss=3.907978\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=3.8401938915252685\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] Epoch[48] Batch [10]#011Speed: 2803.82 samples/sec#011loss=3.840194\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916066.9089622, \"EndTime\": 1680916067.3093758, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 399.9030590057373, \"count\": 1, \"min\": 399.9030590057373, \"max\": 399.9030590057373}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1674.816917525128 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] #progress_metric: host=algo-1, completed 12.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] #quality_metric: host=algo-1, epoch=48, train loss <loss>=3.9079781012101606\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] Epoch[49] Batch[0] avg_epoch_loss=4.114411\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=4.114411354064941\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] Epoch[49] Batch[5] avg_epoch_loss=3.974360\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=3.9743602673212686\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] Epoch[49] Batch [5]#011Speed: 2829.81 samples/sec#011loss=3.974360\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] Epoch[49] Batch[10] avg_epoch_loss=3.993992\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=4.017548990249634\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] Epoch[49] Batch [10]#011Speed: 2433.60 samples/sec#011loss=4.017549\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916067.309476, \"EndTime\": 1680916067.7881415, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 478.1661033630371, \"count\": 1, \"min\": 478.1661033630371, \"max\": 478.1661033630371}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1430.048847598245 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] #quality_metric: host=algo-1, epoch=49, train loss <loss>=3.99399150501598\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] Epoch[50] Batch[0] avg_epoch_loss=4.598212\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:47 INFO 140448693614400] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=4.598212242126465\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] Epoch[50] Batch[5] avg_epoch_loss=4.324783\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=4.324783404668172\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] Epoch[50] Batch [5]#011Speed: 3077.29 samples/sec#011loss=4.324783\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] Epoch[50] Batch[10] avg_epoch_loss=4.301803\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=4.274226951599121\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] Epoch[50] Batch [10]#011Speed: 2673.67 samples/sec#011loss=4.274227\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916067.7882388, \"EndTime\": 1680916068.1945524, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 405.73787689208984, \"count\": 1, \"min\": 405.73787689208984, \"max\": 405.73787689208984}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1670.5093360934914 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] #progress_metric: host=algo-1, completed 12.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] #quality_metric: host=algo-1, epoch=50, train loss <loss>=4.301803198727694\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] Epoch[51] Batch[0] avg_epoch_loss=3.637833\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=3.6378331184387207\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] Epoch[51] Batch[5] avg_epoch_loss=4.067519\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=4.067518552144368\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] Epoch[51] Batch [5]#011Speed: 2857.16 samples/sec#011loss=4.067519\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] Epoch[51] Batch[10] avg_epoch_loss=3.932129\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=3.769661808013916\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] Epoch[51] Batch [10]#011Speed: 2872.09 samples/sec#011loss=3.769662\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916068.1946383, \"EndTime\": 1680916068.6447163, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 449.59330558776855, \"count\": 1, \"min\": 449.59330558776855, \"max\": 449.59330558776855}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1463.095690528068 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] #quality_metric: host=algo-1, epoch=51, train loss <loss>=3.9321291229941626\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] Epoch[52] Batch[0] avg_epoch_loss=4.290679\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=4.290679454803467\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] Epoch[52] Batch[5] avg_epoch_loss=4.189927\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=4.1899270216623945\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:48 INFO 140448693614400] Epoch[52] Batch [5]#011Speed: 3004.60 samples/sec#011loss=4.189927\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] Epoch[52] Batch[10] avg_epoch_loss=4.051875\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=3.8862127304077148\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] Epoch[52] Batch [10]#011Speed: 2539.41 samples/sec#011loss=3.886213\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916068.644812, \"EndTime\": 1680916069.0572748, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 411.8790626525879, \"count\": 1, \"min\": 411.8790626525879, \"max\": 411.8790626525879}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1648.00317572193 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] #progress_metric: host=algo-1, completed 13.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] #quality_metric: host=algo-1, epoch=52, train loss <loss>=4.051875071092085\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] Epoch[53] Batch[0] avg_epoch_loss=4.177026\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=4.17702579498291\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] Epoch[53] Batch[5] avg_epoch_loss=3.964699\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=3.9646991888682046\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] Epoch[53] Batch [5]#011Speed: 2925.35 samples/sec#011loss=3.964699\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] Epoch[53] Batch[10] avg_epoch_loss=4.200043\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] #quality_metric: host=algo-1, epoch=53, batch=10 train loss <loss>=4.482455444335938\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] Epoch[53] Batch [10]#011Speed: 2863.48 samples/sec#011loss=4.482455\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916069.0573704, \"EndTime\": 1680916069.4619808, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 403.9914608001709, \"count\": 1, \"min\": 403.9914608001709, \"max\": 403.9914608001709}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1633.1427418484177 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] #quality_metric: host=algo-1, epoch=53, train loss <loss>=4.200042941353538\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] Epoch[54] Batch[0] avg_epoch_loss=3.723667\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=3.7236673831939697\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] Epoch[54] Batch[5] avg_epoch_loss=3.855121\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=3.855120539665222\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] Epoch[54] Batch [5]#011Speed: 3029.08 samples/sec#011loss=3.855121\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] Epoch[54] Batch[10] avg_epoch_loss=3.993602\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=4.159780836105346\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] Epoch[54] Batch [10]#011Speed: 2630.60 samples/sec#011loss=4.159781\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916069.4620745, \"EndTime\": 1680916069.8630238, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 400.35295486450195, \"count\": 1, \"min\": 400.35295486450195, \"max\": 400.35295486450195}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1692.9723342606644 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] #progress_metric: host=algo-1, completed 13.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] #quality_metric: host=algo-1, epoch=54, train loss <loss>=3.9936024925925513\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:49 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] Epoch[55] Batch[0] avg_epoch_loss=4.509596\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=4.509596347808838\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] Epoch[55] Batch[5] avg_epoch_loss=3.957222\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=3.957221786181132\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] Epoch[55] Batch [5]#011Speed: 3046.16 samples/sec#011loss=3.957222\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] Epoch[55] Batch[10] avg_epoch_loss=4.017302\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=4.089397668838501\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] Epoch[55] Batch [10]#011Speed: 2731.19 samples/sec#011loss=4.089398\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916069.863111, \"EndTime\": 1680916070.3172963, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 453.71413230895996, \"count\": 1, \"min\": 453.71413230895996, \"max\": 453.71413230895996}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1498.3355765341234 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] #quality_metric: host=algo-1, epoch=55, train loss <loss>=4.017301732843572\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] Epoch[56] Batch[0] avg_epoch_loss=3.675641\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=3.6756410598754883\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] Epoch[56] Batch[5] avg_epoch_loss=4.109866\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=4.109865824381511\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] Epoch[56] Batch [5]#011Speed: 3088.93 samples/sec#011loss=4.109866\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] Epoch[56] Batch[10] avg_epoch_loss=4.163348\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=4.227526903152466\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] Epoch[56] Batch [10]#011Speed: 2238.08 samples/sec#011loss=4.227527\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] processed a total of 707 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916070.31738, \"EndTime\": 1680916070.8207493, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 502.75444984436035, \"count\": 1, \"min\": 502.75444984436035, \"max\": 502.75444984436035}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1405.6958020725835 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] #progress_metric: host=algo-1, completed 14.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] #quality_metric: host=algo-1, epoch=56, train loss <loss>=4.302578270435333\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:50 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] Epoch[57] Batch[0] avg_epoch_loss=3.869967\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=3.869967222213745\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] Epoch[57] Batch[5] avg_epoch_loss=3.932445\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=3.9324451684951782\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] Epoch[57] Batch [5]#011Speed: 2589.31 samples/sec#011loss=3.932445\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916070.820902, \"EndTime\": 1680916071.2207294, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 399.0912437438965, \"count\": 1, \"min\": 399.0912437438965, \"max\": 399.0912437438965}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1545.5017925149734 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] #quality_metric: host=algo-1, epoch=57, train loss <loss>=4.013338351249695\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] Epoch[58] Batch[0] avg_epoch_loss=4.206429\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=4.206429481506348\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] Epoch[58] Batch[5] avg_epoch_loss=4.009976\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=4.009975949923198\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] Epoch[58] Batch [5]#011Speed: 2927.71 samples/sec#011loss=4.009976\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] Epoch[58] Batch[10] avg_epoch_loss=4.050499\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=4.099127197265625\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] Epoch[58] Batch [10]#011Speed: 2736.16 samples/sec#011loss=4.099127\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916071.2208123, \"EndTime\": 1680916071.6251268, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 403.705358505249, \"count\": 1, \"min\": 403.705358505249, \"max\": 403.705358505249}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1639.1631347439773 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] #progress_metric: host=algo-1, completed 14.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] #quality_metric: host=algo-1, epoch=58, train loss <loss>=4.050499244169756\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] Epoch[59] Batch[0] avg_epoch_loss=4.283811\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=4.283811092376709\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] Epoch[59] Batch[5] avg_epoch_loss=3.995610\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=3.9956099589665732\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:51 INFO 140448693614400] Epoch[59] Batch [5]#011Speed: 2423.56 samples/sec#011loss=3.995610\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] Epoch[59] Batch[10] avg_epoch_loss=3.960520\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=3.9184126377105715\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] Epoch[59] Batch [10]#011Speed: 2407.75 samples/sec#011loss=3.918413\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916071.6252384, \"EndTime\": 1680916072.0781748, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 452.3320198059082, \"count\": 1, \"min\": 452.3320198059082, \"max\": 452.3320198059082}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1432.1472452636915 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] #quality_metric: host=algo-1, epoch=59, train loss <loss>=3.9605202674865723\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] Epoch[60] Batch[0] avg_epoch_loss=3.490502\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=3.490501642227173\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] Epoch[60] Batch[5] avg_epoch_loss=3.916296\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=3.916296402613322\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] Epoch[60] Batch [5]#011Speed: 3035.58 samples/sec#011loss=3.916296\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] Epoch[60] Batch[10] avg_epoch_loss=4.015289\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=4.134079027175903\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] Epoch[60] Batch [10]#011Speed: 2358.69 samples/sec#011loss=4.134079\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916072.0782714, \"EndTime\": 1680916072.5065184, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 427.60396003723145, \"count\": 1, \"min\": 427.60396003723145, \"max\": 427.60396003723145}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1535.8450498793632 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] #progress_metric: host=algo-1, completed 15.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] #quality_metric: host=algo-1, epoch=60, train loss <loss>=4.015288504687223\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] Epoch[61] Batch[0] avg_epoch_loss=3.979561\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=3.9795610904693604\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] Epoch[61] Batch[5] avg_epoch_loss=4.074351\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=4.074350953102112\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] Epoch[61] Batch [5]#011Speed: 2790.01 samples/sec#011loss=4.074351\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] Epoch[61] Batch[10] avg_epoch_loss=3.967593\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=3.8394829273223876\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] Epoch[61] Batch [10]#011Speed: 2455.87 samples/sec#011loss=3.839483\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] processed a total of 721 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916072.5066388, \"EndTime\": 1680916072.9769652, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 469.5708751678467, \"count\": 1, \"min\": 469.5708751678467, \"max\": 469.5708751678467}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1534.9573656395357 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] #quality_metric: host=algo-1, epoch=61, train loss <loss>=4.065571228663127\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:52 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] Epoch[62] Batch[0] avg_epoch_loss=4.706831\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=4.706831455230713\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] Epoch[62] Batch[5] avg_epoch_loss=4.143375\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=4.143374641736348\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] Epoch[62] Batch [5]#011Speed: 2604.32 samples/sec#011loss=4.143375\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] Epoch[62] Batch[10] avg_epoch_loss=4.174547\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=4.211953783035279\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] Epoch[62] Batch [10]#011Speed: 2522.31 samples/sec#011loss=4.211954\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] processed a total of 707 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916072.9770656, \"EndTime\": 1680916073.4316978, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 453.66573333740234, \"count\": 1, \"min\": 453.66573333740234, \"max\": 453.66573333740234}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1557.970725392412 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] #progress_metric: host=algo-1, completed 15.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] #quality_metric: host=algo-1, epoch=62, train loss <loss>=3.9614597062269845\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] Epoch[63] Batch[0] avg_epoch_loss=3.739858\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=3.7398581504821777\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] Epoch[63] Batch[5] avg_epoch_loss=3.929978\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=3.929977774620056\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] Epoch[63] Batch [5]#011Speed: 2919.24 samples/sec#011loss=3.929978\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] Epoch[63] Batch[10] avg_epoch_loss=3.983389\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=4.047481489181519\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] Epoch[63] Batch [10]#011Speed: 2637.29 samples/sec#011loss=4.047481\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916073.4317827, \"EndTime\": 1680916073.8471155, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 414.7040843963623, \"count\": 1, \"min\": 414.7040843963623, \"max\": 414.7040843963623}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1646.4009315033538 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] #quality_metric: host=algo-1, epoch=63, train loss <loss>=3.9833885539661753\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:53 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] Epoch[64] Batch[0] avg_epoch_loss=4.102653\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=4.1026530265808105\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] Epoch[64] Batch[5] avg_epoch_loss=3.956615\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=3.956615447998047\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] Epoch[64] Batch [5]#011Speed: 2795.48 samples/sec#011loss=3.956615\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] Epoch[64] Batch[10] avg_epoch_loss=3.953112\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=3.9489081382751463\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] Epoch[64] Batch [10]#011Speed: 2539.91 samples/sec#011loss=3.948908\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] processed a total of 714 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916073.8472028, \"EndTime\": 1680916074.3407807, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 493.060827255249, \"count\": 1, \"min\": 493.060827255249, \"max\": 493.060827255249}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1447.4889536356236 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] #progress_metric: host=algo-1, completed 16.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] #quality_metric: host=algo-1, epoch=64, train loss <loss>=3.91185071070989\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] Epoch[65] Batch[0] avg_epoch_loss=3.595920\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=3.5959198474884033\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] Epoch[65] Batch[5] avg_epoch_loss=3.914816\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=3.9148160219192505\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] Epoch[65] Batch [5]#011Speed: 2713.61 samples/sec#011loss=3.914816\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] Epoch[65] Batch[10] avg_epoch_loss=3.817743\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=3.701254940032959\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] Epoch[65] Batch [10]#011Speed: 2404.74 samples/sec#011loss=3.701255\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] processed a total of 690 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916074.3409393, \"EndTime\": 1680916074.765399, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 423.6938953399658, \"count\": 1, \"min\": 423.6938953399658, \"max\": 423.6938953399658}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1627.484685760752 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] #quality_metric: host=algo-1, epoch=65, train loss <loss>=3.817742802880027\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] Epoch[66] Batch[0] avg_epoch_loss=3.892737\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:54 INFO 140448693614400] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=3.89273738861084\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] Epoch[66] Batch[5] avg_epoch_loss=3.730737\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=3.730737010637919\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] Epoch[66] Batch [5]#011Speed: 2816.24 samples/sec#011loss=3.730737\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916074.7656224, \"EndTime\": 1680916075.15333, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 387.0265483856201, \"count\": 1, \"min\": 387.0265483856201, \"max\": 387.0265483856201}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1632.3746456045105 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] #progress_metric: host=algo-1, completed 16.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] #quality_metric: host=algo-1, epoch=66, train loss <loss>=3.8125605821609496\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] Epoch[67] Batch[0] avg_epoch_loss=4.306311\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=4.306310653686523\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] Epoch[67] Batch[5] avg_epoch_loss=3.868381\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=3.8683809836705527\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] Epoch[67] Batch [5]#011Speed: 3090.03 samples/sec#011loss=3.868381\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] Epoch[67] Batch[10] avg_epoch_loss=4.059732\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=4.2893531799316404\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] Epoch[67] Batch [10]#011Speed: 2808.34 samples/sec#011loss=4.289353\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916075.1534252, \"EndTime\": 1680916075.552422, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 398.19860458374023, \"count\": 1, \"min\": 398.19860458374023, \"max\": 398.19860458374023}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1691.7060352312978 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] #quality_metric: host=algo-1, epoch=67, train loss <loss>=4.059731981971047\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] Epoch[68] Batch[0] avg_epoch_loss=3.866761\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=3.866760730743408\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] Epoch[68] Batch[5] avg_epoch_loss=3.972296\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=3.9722960392634072\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] Epoch[68] Batch [5]#011Speed: 2819.25 samples/sec#011loss=3.972296\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] Epoch[68] Batch[10] avg_epoch_loss=4.106636\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=4.267843770980835\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] Epoch[68] Batch [10]#011Speed: 2731.11 samples/sec#011loss=4.267844\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916075.5525854, \"EndTime\": 1680916075.9611933, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 407.9906940460205, \"count\": 1, \"min\": 407.9906940460205, \"max\": 407.9906940460205}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1575.4446174857962 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] #progress_metric: host=algo-1, completed 17.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] #quality_metric: host=algo-1, epoch=68, train loss <loss>=4.106635917316783\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:55 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] Epoch[69] Batch[0] avg_epoch_loss=3.512432\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=3.5124316215515137\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] Epoch[69] Batch[5] avg_epoch_loss=3.719766\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=3.719766060511271\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] Epoch[69] Batch [5]#011Speed: 2810.74 samples/sec#011loss=3.719766\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] Epoch[69] Batch[10] avg_epoch_loss=3.819659\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=3.9395299434661863\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] Epoch[69] Batch [10]#011Speed: 2278.80 samples/sec#011loss=3.939530\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] processed a total of 741 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916075.9612956, \"EndTime\": 1680916076.4248297, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 463.0300998687744, \"count\": 1, \"min\": 463.0300998687744, \"max\": 463.0300998687744}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1599.594881879835 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] #quality_metric: host=algo-1, epoch=69, train loss <loss>=3.898586372534434\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] Epoch[70] Batch[0] avg_epoch_loss=3.854572\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=3.854572057723999\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] Epoch[70] Batch[5] avg_epoch_loss=3.903049\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=3.903048555056254\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] Epoch[70] Batch [5]#011Speed: 2554.16 samples/sec#011loss=3.903049\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] Epoch[70] Batch[10] avg_epoch_loss=4.028061\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=4.178076505661011\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] Epoch[70] Batch [10]#011Speed: 2934.75 samples/sec#011loss=4.178077\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916076.4249945, \"EndTime\": 1680916076.84033, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 414.6442413330078, \"count\": 1, \"min\": 414.6442413330078, \"max\": 414.6442413330078}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1547.7919403678268 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] #progress_metric: host=algo-1, completed 17.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] #quality_metric: host=algo-1, epoch=70, train loss <loss>=4.028061259876598\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:56 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] Epoch[71] Batch[0] avg_epoch_loss=3.868016\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=3.8680155277252197\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] Epoch[71] Batch[5] avg_epoch_loss=3.911666\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=3.9116660356521606\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] Epoch[71] Batch [5]#011Speed: 2572.66 samples/sec#011loss=3.911666\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] Epoch[71] Batch[10] avg_epoch_loss=3.774963\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=3.610918927192688\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] Epoch[71] Batch [10]#011Speed: 2682.16 samples/sec#011loss=3.610919\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916076.8404267, \"EndTime\": 1680916077.268872, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 427.83260345458984, \"count\": 1, \"min\": 427.83260345458984, \"max\": 427.83260345458984}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1556.220502129812 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] #quality_metric: host=algo-1, epoch=71, train loss <loss>=3.7749628045342187\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] Epoch[72] Batch[0] avg_epoch_loss=3.854165\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=3.8541650772094727\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] Epoch[72] Batch[5] avg_epoch_loss=4.071316\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=4.071316043535869\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] Epoch[72] Batch [5]#011Speed: 2527.68 samples/sec#011loss=4.071316\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] Epoch[72] Batch[10] avg_epoch_loss=4.112241\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=4.161350297927856\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] Epoch[72] Batch [10]#011Speed: 2351.37 samples/sec#011loss=4.161350\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916077.2689636, \"EndTime\": 1680916077.7703116, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 500.7748603820801, \"count\": 1, \"min\": 500.7748603820801, \"max\": 500.7748603820801}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1353.5010128394536 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] #progress_metric: host=algo-1, completed 18.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] #quality_metric: host=algo-1, epoch=72, train loss <loss>=4.112240704623136\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] Epoch[73] Batch[0] avg_epoch_loss=3.986962\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:57 INFO 140448693614400] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=3.986962080001831\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] Epoch[73] Batch[5] avg_epoch_loss=3.901207\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=3.901206930478414\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] Epoch[73] Batch [5]#011Speed: 2889.94 samples/sec#011loss=3.901207\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916077.770416, \"EndTime\": 1680916078.1801105, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 409.0907573699951, \"count\": 1, \"min\": 409.0907573699951, \"max\": 409.0907573699951}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1529.7021466003664 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] #quality_metric: host=algo-1, epoch=73, train loss <loss>=3.919720721244812\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] Epoch[74] Batch[0] avg_epoch_loss=3.772826\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=3.7728261947631836\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] Epoch[74] Batch[5] avg_epoch_loss=3.953579\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=3.9535786708196006\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] Epoch[74] Batch [5]#011Speed: 3079.61 samples/sec#011loss=3.953579\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] Epoch[74] Batch[10] avg_epoch_loss=3.846425\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=3.7178407192230223\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] Epoch[74] Batch [10]#011Speed: 2620.41 samples/sec#011loss=3.717841\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916078.1802044, \"EndTime\": 1680916078.591684, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 410.646915435791, \"count\": 1, \"min\": 410.646915435791, \"max\": 410.646915435791}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1665.0525945568543 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] #progress_metric: host=algo-1, completed 18.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] #quality_metric: host=algo-1, epoch=74, train loss <loss>=3.8464250564575195\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] Epoch[75] Batch[0] avg_epoch_loss=4.021849\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=4.021849155426025\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] Epoch[75] Batch[5] avg_epoch_loss=4.027046\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=4.027045925458272\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:58 INFO 140448693614400] Epoch[75] Batch [5]#011Speed: 3067.46 samples/sec#011loss=4.027046\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] Epoch[75] Batch[10] avg_epoch_loss=3.838476\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=3.6121928691864014\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] Epoch[75] Batch [10]#011Speed: 2838.12 samples/sec#011loss=3.612193\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916078.5917869, \"EndTime\": 1680916079.0398967, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 447.4172592163086, \"count\": 1, \"min\": 447.4172592163086, \"max\": 447.4172592163086}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1480.5680034330487 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] #quality_metric: host=algo-1, epoch=75, train loss <loss>=3.8384763544256035\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] Epoch[76] Batch[0] avg_epoch_loss=4.054096\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=4.05409574508667\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] Epoch[76] Batch[5] avg_epoch_loss=3.941531\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=3.94153102238973\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] Epoch[76] Batch [5]#011Speed: 3083.81 samples/sec#011loss=3.941531\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] Epoch[76] Batch[10] avg_epoch_loss=3.982850\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=4.032432889938354\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] Epoch[76] Batch [10]#011Speed: 2925.93 samples/sec#011loss=4.032433\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916079.0402226, \"EndTime\": 1680916079.4355638, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 394.60158348083496, \"count\": 1, \"min\": 394.60158348083496, \"max\": 394.60158348083496}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1644.0760522412218 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] #progress_metric: host=algo-1, completed 19.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] #quality_metric: host=algo-1, epoch=76, train loss <loss>=3.98285005309365\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] Epoch[77] Batch[0] avg_epoch_loss=4.270037\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=4.270036697387695\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] Epoch[77] Batch[5] avg_epoch_loss=4.229960\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=4.229959646860759\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] Epoch[77] Batch [5]#011Speed: 3067.59 samples/sec#011loss=4.229960\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] Epoch[77] Batch[10] avg_epoch_loss=4.078367\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=3.8964560985565186\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] Epoch[77] Batch [10]#011Speed: 2695.12 samples/sec#011loss=3.896456\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916079.4356713, \"EndTime\": 1680916079.8973427, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 461.026668548584, \"count\": 1, \"min\": 461.026668548584, \"max\": 461.026668548584}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1474.3391201501981 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] #quality_metric: host=algo-1, epoch=77, train loss <loss>=4.078367124904286\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:07:59 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] Epoch[78] Batch[0] avg_epoch_loss=3.540007\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=3.5400071144104004\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] Epoch[78] Batch[5] avg_epoch_loss=3.819696\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=3.8196961085001626\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] Epoch[78] Batch [5]#011Speed: 2670.35 samples/sec#011loss=3.819696\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] Epoch[78] Batch[10] avg_epoch_loss=3.820886\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=3.822313594818115\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] Epoch[78] Batch [10]#011Speed: 2564.54 samples/sec#011loss=3.822314\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] processed a total of 693 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916079.8974977, \"EndTime\": 1680916080.3173702, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 419.24118995666504, \"count\": 1, \"min\": 419.24118995666504, \"max\": 419.24118995666504}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1652.4395138848224 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] #progress_metric: host=algo-1, completed 19.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] #quality_metric: host=algo-1, epoch=78, train loss <loss>=3.8208858750083228\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] Epoch[79] Batch[0] avg_epoch_loss=3.613499\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=3.613499402999878\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] Epoch[79] Batch[5] avg_epoch_loss=3.781520\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=3.7815200090408325\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] Epoch[79] Batch [5]#011Speed: 3066.25 samples/sec#011loss=3.781520\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] Epoch[79] Batch[10] avg_epoch_loss=3.663786\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=3.5225042343139648\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] Epoch[79] Batch [10]#011Speed: 2815.13 samples/sec#011loss=3.522504\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916080.3174577, \"EndTime\": 1680916080.7192292, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 401.19385719299316, \"count\": 1, \"min\": 401.19385719299316, \"max\": 401.19385719299316}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1639.2573480977358 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] #quality_metric: host=algo-1, epoch=79, train loss <loss>=3.6637855659831655\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_fc7774d9-b54d-42c5-9e70-c2735712af44-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916080.7193959, \"EndTime\": 1680916080.7346153, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 14.589548110961914, \"count\": 1, \"min\": 14.589548110961914, \"max\": 14.589548110961914}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] Epoch[80] Batch[0] avg_epoch_loss=3.550393\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:00 INFO 140448693614400] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=3.5503931045532227\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] Epoch[80] Batch[5] avg_epoch_loss=3.867393\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=3.8673928578694663\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] Epoch[80] Batch [5]#011Speed: 2814.47 samples/sec#011loss=3.867393\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] Epoch[80] Batch[10] avg_epoch_loss=3.904855\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=3.9498106479644775\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] Epoch[80] Batch [10]#011Speed: 1953.59 samples/sec#011loss=3.949811\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916080.7346878, \"EndTime\": 1680916081.2439268, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 509.17673110961914, \"count\": 1, \"min\": 509.17673110961914, \"max\": 509.17673110961914}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1266.330928920823 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] #progress_metric: host=algo-1, completed 20.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] #quality_metric: host=algo-1, epoch=80, train loss <loss>=3.904855489730835\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] Epoch[81] Batch[0] avg_epoch_loss=3.858480\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=3.858480215072632\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] Epoch[81] Batch[5] avg_epoch_loss=3.812602\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=3.812602480252584\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] Epoch[81] Batch [5]#011Speed: 2229.09 samples/sec#011loss=3.812602\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916081.244057, \"EndTime\": 1680916081.7421026, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 497.47371673583984, \"count\": 1, \"min\": 497.47371673583984, \"max\": 497.47371673583984}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1271.9821893374613 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] #quality_metric: host=algo-1, epoch=81, train loss <loss>=3.8820189476013183\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:01 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] Epoch[82] Batch[0] avg_epoch_loss=3.728399\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=3.728398561477661\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] Epoch[82] Batch[5] avg_epoch_loss=3.732635\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=3.732634981473287\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] Epoch[82] Batch [5]#011Speed: 2215.89 samples/sec#011loss=3.732635\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] Epoch[82] Batch[10] avg_epoch_loss=3.611666\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=3.4665030956268312\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] Epoch[82] Batch [10]#011Speed: 2197.49 samples/sec#011loss=3.466503\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916081.7422333, \"EndTime\": 1680916082.3065186, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 563.2016658782959, \"count\": 1, \"min\": 563.2016658782959, \"max\": 563.2016658782959}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1139.23188562764 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] #progress_metric: host=algo-1, completed 20.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] #quality_metric: host=algo-1, epoch=82, train loss <loss>=3.6116659424521704\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_81014c13-e370-4952-92dc-5541ade2938e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916082.3066187, \"EndTime\": 1680916082.3224823, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 15.330791473388672, \"count\": 1, \"min\": 15.330791473388672, \"max\": 15.330791473388672}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] Epoch[83] Batch[0] avg_epoch_loss=3.874725\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=3.874724864959717\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] Epoch[83] Batch[5] avg_epoch_loss=3.756681\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=3.7566808462142944\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] Epoch[83] Batch [5]#011Speed: 2083.04 samples/sec#011loss=3.756681\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916082.3225658, \"EndTime\": 1680916082.7930202, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 470.38841247558594, \"count\": 1, \"min\": 470.38841247558594, \"max\": 470.38841247558594}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1283.6359977117736 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] #quality_metric: host=algo-1, epoch=83, train loss <loss>=3.658217978477478\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:02 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] Epoch[84] Batch[0] avg_epoch_loss=3.683522\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=3.6835217475891113\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] Epoch[84] Batch[5] avg_epoch_loss=3.994541\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=3.994541128476461\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] Epoch[84] Batch [5]#011Speed: 2182.56 samples/sec#011loss=3.994541\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916082.793128, \"EndTime\": 1680916083.3151567, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 521.0103988647461, \"count\": 1, \"min\": 521.0103988647461, \"max\": 521.0103988647461}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1226.1409776517144 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] #progress_metric: host=algo-1, completed 21.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] #quality_metric: host=algo-1, epoch=84, train loss <loss>=3.9840309858322143\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] Epoch[85] Batch[0] avg_epoch_loss=3.817127\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=3.817127227783203\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] Epoch[85] Batch[5] avg_epoch_loss=4.037134\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=4.0371343692143755\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] Epoch[85] Batch [5]#011Speed: 2018.91 samples/sec#011loss=4.037134\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] Epoch[85] Batch[10] avg_epoch_loss=3.815628\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=3.5498213291168215\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] Epoch[85] Batch [10]#011Speed: 2178.22 samples/sec#011loss=3.549821\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916083.315242, \"EndTime\": 1680916083.8869162, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 570.6892013549805, \"count\": 1, \"min\": 570.6892013549805, \"max\": 570.6892013549805}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1159.721850122378 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] #quality_metric: host=algo-1, epoch=85, train loss <loss>=3.8156284418973057\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:03 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] Epoch[86] Batch[0] avg_epoch_loss=4.182471\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=4.18247127532959\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] Epoch[86] Batch[5] avg_epoch_loss=3.918694\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=3.9186940590540567\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] Epoch[86] Batch [5]#011Speed: 2439.68 samples/sec#011loss=3.918694\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] Epoch[86] Batch[10] avg_epoch_loss=3.928752\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=3.940821123123169\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] Epoch[86] Batch [10]#011Speed: 2300.45 samples/sec#011loss=3.940821\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916083.887011, \"EndTime\": 1680916084.344676, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 457.05246925354004, \"count\": 1, \"min\": 457.05246925354004, \"max\": 457.05246925354004}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1445.6699248234 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] #progress_metric: host=algo-1, completed 21.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] #quality_metric: host=algo-1, epoch=86, train loss <loss>=3.928751815449108\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] Epoch[87] Batch[0] avg_epoch_loss=3.786972\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=3.7869715690612793\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] Epoch[87] Batch[5] avg_epoch_loss=3.913270\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=3.9132702350616455\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] Epoch[87] Batch [5]#011Speed: 2831.47 samples/sec#011loss=3.913270\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] Epoch[87] Batch[10] avg_epoch_loss=4.014649\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] #quality_metric: host=algo-1, epoch=87, batch=10 train loss <loss>=4.1363026142120365\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] Epoch[87] Batch [10]#011Speed: 2612.16 samples/sec#011loss=4.136303\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916084.344803, \"EndTime\": 1680916084.7666047, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 421.1843013763428, \"count\": 1, \"min\": 421.1843013763428, \"max\": 421.1843013763428}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1557.0149642925858 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] #quality_metric: host=algo-1, epoch=87, train loss <loss>=4.014648589220914\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] Epoch[88] Batch[0] avg_epoch_loss=4.000300\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:04 INFO 140448693614400] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=4.000300407409668\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] Epoch[88] Batch[5] avg_epoch_loss=3.986156\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=3.98615562915802\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] Epoch[88] Batch [5]#011Speed: 2540.24 samples/sec#011loss=3.986156\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] Epoch[88] Batch[10] avg_epoch_loss=3.896965\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=3.78993706703186\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] Epoch[88] Batch [10]#011Speed: 2600.19 samples/sec#011loss=3.789937\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] processed a total of 691 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916084.766697, \"EndTime\": 1680916085.2041402, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 436.91563606262207, \"count\": 1, \"min\": 436.91563606262207, \"max\": 436.91563606262207}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1581.0242357988081 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] #progress_metric: host=algo-1, completed 22.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] #quality_metric: host=algo-1, epoch=88, train loss <loss>=3.8969653736461294\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] Epoch[89] Batch[0] avg_epoch_loss=3.741761\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=3.741760730743408\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] Epoch[89] Batch[5] avg_epoch_loss=4.128950\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=4.128950198491414\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] Epoch[89] Batch [5]#011Speed: 2481.44 samples/sec#011loss=4.128950\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] Epoch[89] Batch[10] avg_epoch_loss=3.849757\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=3.514725399017334\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] Epoch[89] Batch [10]#011Speed: 2658.67 samples/sec#011loss=3.514725\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916085.2042356, \"EndTime\": 1680916085.6356473, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 430.70125579833984, \"count\": 1, \"min\": 430.70125579833984, \"max\": 430.70125579833984}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1508.6197193698133 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] #quality_metric: host=algo-1, epoch=89, train loss <loss>=3.849757107821378\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] Epoch[90] Batch[0] avg_epoch_loss=4.250656\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=4.250655651092529\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] Epoch[90] Batch[5] avg_epoch_loss=4.208311\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=4.208310842514038\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:05 INFO 140448693614400] Epoch[90] Batch [5]#011Speed: 2904.88 samples/sec#011loss=4.208311\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] Epoch[90] Batch[10] avg_epoch_loss=4.258443\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] #quality_metric: host=algo-1, epoch=90, batch=10 train loss <loss>=4.318601751327515\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] Epoch[90] Batch [10]#011Speed: 2508.56 samples/sec#011loss=4.318602\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] processed a total of 708 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916085.6357558, \"EndTime\": 1680916086.081092, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 444.53883171081543, \"count\": 1, \"min\": 444.53883171081543, \"max\": 444.53883171081543}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1591.8413115682083 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] #progress_metric: host=algo-1, completed 22.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] #quality_metric: host=algo-1, epoch=90, train loss <loss>=4.391407390435536\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] Epoch[91] Batch[0] avg_epoch_loss=3.969038\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=3.9690380096435547\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] Epoch[91] Batch[5] avg_epoch_loss=3.953667\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=3.9536670049031577\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] Epoch[91] Batch [5]#011Speed: 3095.22 samples/sec#011loss=3.953667\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] Epoch[91] Batch[10] avg_epoch_loss=4.077375\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=4.225824594497681\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] Epoch[91] Batch [10]#011Speed: 2684.37 samples/sec#011loss=4.225825\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916086.081277, \"EndTime\": 1680916086.538705, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 456.7131996154785, \"count\": 1, \"min\": 456.7131996154785, \"max\": 456.7131996154785}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1455.5985713361263 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] #quality_metric: host=algo-1, epoch=91, train loss <loss>=4.0773750001733955\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] Epoch[92] Batch[0] avg_epoch_loss=3.712917\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=3.7129173278808594\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] Epoch[92] Batch[5] avg_epoch_loss=3.954388\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=3.95438814163208\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] Epoch[92] Batch [5]#011Speed: 3084.97 samples/sec#011loss=3.954388\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] Epoch[92] Batch[10] avg_epoch_loss=3.728033\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=3.4564078092575072\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] Epoch[92] Batch [10]#011Speed: 2718.46 samples/sec#011loss=3.456408\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916086.5388076, \"EndTime\": 1680916086.9953873, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 456.0427665710449, \"count\": 1, \"min\": 456.0427665710449, \"max\": 456.0427665710449}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1444.5565776916958 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] #progress_metric: host=algo-1, completed 23.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] #quality_metric: host=algo-1, epoch=92, train loss <loss>=3.7280334450981836\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:06 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] Epoch[93] Batch[0] avg_epoch_loss=3.858988\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=3.8589882850646973\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] Epoch[93] Batch[5] avg_epoch_loss=3.810628\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=3.8106284936269126\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] Epoch[93] Batch [5]#011Speed: 3075.49 samples/sec#011loss=3.810628\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] Epoch[93] Batch[10] avg_epoch_loss=3.679392\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=3.5219091415405273\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] Epoch[93] Batch [10]#011Speed: 2798.93 samples/sec#011loss=3.521909\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916086.9954994, \"EndTime\": 1680916087.4486465, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 452.61406898498535, \"count\": 1, \"min\": 452.61406898498535, \"max\": 452.61406898498535}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1424.6063742003023 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] #quality_metric: host=algo-1, epoch=93, train loss <loss>=3.6793924244967373\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] Epoch[94] Batch[0] avg_epoch_loss=3.812303\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=3.812303304672241\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] Epoch[94] Batch[5] avg_epoch_loss=3.736118\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=3.7361175616582236\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] Epoch[94] Batch [5]#011Speed: 2662.97 samples/sec#011loss=3.736118\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] Epoch[94] Batch[10] avg_epoch_loss=3.754270\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=3.7760519981384277\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] Epoch[94] Batch [10]#011Speed: 2634.57 samples/sec#011loss=3.776052\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916087.448742, \"EndTime\": 1680916087.878692, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 429.2774200439453, \"count\": 1, \"min\": 429.2774200439453, \"max\": 429.2774200439453}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1571.8693560019522 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] #progress_metric: host=algo-1, completed 23.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] #quality_metric: host=algo-1, epoch=94, train loss <loss>=3.7542695782401343\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:07 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] Epoch[95] Batch[0] avg_epoch_loss=3.528082\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=3.5280821323394775\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] Epoch[95] Batch[5] avg_epoch_loss=3.661716\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=3.661715825398763\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] Epoch[95] Batch [5]#011Speed: 3007.62 samples/sec#011loss=3.661716\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916087.878794, \"EndTime\": 1680916088.2807739, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 401.33023262023926, \"count\": 1, \"min\": 401.33023262023926, \"max\": 401.33023262023926}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1581.6818764612715 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] #quality_metric: host=algo-1, epoch=95, train loss <loss>=3.7568250417709352\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] Epoch[96] Batch[0] avg_epoch_loss=3.781049\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=3.7810490131378174\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] Epoch[96] Batch[5] avg_epoch_loss=3.703696\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=3.703695774078369\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] Epoch[96] Batch [5]#011Speed: 3066.87 samples/sec#011loss=3.703696\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] Epoch[96] Batch[10] avg_epoch_loss=3.640201\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] #quality_metric: host=algo-1, epoch=96, batch=10 train loss <loss>=3.564008092880249\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] Epoch[96] Batch [10]#011Speed: 2437.37 samples/sec#011loss=3.564008\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916088.2808678, \"EndTime\": 1680916088.6981816, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 416.67842864990234, \"count\": 1, \"min\": 416.67842864990234, \"max\": 416.67842864990234}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1583.3890388439977 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] #progress_metric: host=algo-1, completed 24.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] #quality_metric: host=algo-1, epoch=96, train loss <loss>=3.640201373533769\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] Epoch[97] Batch[0] avg_epoch_loss=3.939523\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:08 INFO 140448693614400] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=3.9395225048065186\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] Epoch[97] Batch[5] avg_epoch_loss=3.850677\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=3.850677410761515\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] Epoch[97] Batch [5]#011Speed: 2684.92 samples/sec#011loss=3.850677\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] Epoch[97] Batch[10] avg_epoch_loss=3.850131\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] #quality_metric: host=algo-1, epoch=97, batch=10 train loss <loss>=3.8494745254516602\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] Epoch[97] Batch [10]#011Speed: 2639.94 samples/sec#011loss=3.849475\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916088.6982825, \"EndTime\": 1680916089.1247919, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 425.86398124694824, \"count\": 1, \"min\": 425.86398124694824, \"max\": 425.86398124694824}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1598.5902108547523 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] #quality_metric: host=algo-1, epoch=97, train loss <loss>=3.850130644711581\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] Epoch[98] Batch[0] avg_epoch_loss=3.464976\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=3.4649760723114014\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] Epoch[98] Batch[5] avg_epoch_loss=3.688799\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=3.6887991031010947\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] Epoch[98] Batch [5]#011Speed: 2555.90 samples/sec#011loss=3.688799\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] Epoch[98] Batch[10] avg_epoch_loss=3.597833\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=3.4886741638183594\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] Epoch[98] Batch [10]#011Speed: 2588.71 samples/sec#011loss=3.488674\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916089.1248841, \"EndTime\": 1680916089.6095643, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 484.1897487640381, \"count\": 1, \"min\": 484.1897487640381, \"max\": 484.1897487640381}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1356.4879178326971 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] #progress_metric: host=algo-1, completed 24.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] #quality_metric: host=algo-1, epoch=98, train loss <loss>=3.5978332216089424\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_ab4dcedf-9c90-41c3-99f9-008cee16541a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916089.6096654, \"EndTime\": 1680916089.6200426, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.775161743164062, \"count\": 1, \"min\": 9.775161743164062, \"max\": 9.775161743164062}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] Epoch[99] Batch[0] avg_epoch_loss=3.983096\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=3.98309588432312\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] Epoch[99] Batch[5] avg_epoch_loss=3.796906\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=3.7969064315160117\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:09 INFO 140448693614400] Epoch[99] Batch [5]#011Speed: 2765.60 samples/sec#011loss=3.796906\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] Epoch[99] Batch[10] avg_epoch_loss=3.698305\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=3.5799829006195067\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] Epoch[99] Batch [10]#011Speed: 2658.77 samples/sec#011loss=3.579983\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916089.6201262, \"EndTime\": 1680916090.0417876, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 421.5996265411377, \"count\": 1, \"min\": 421.5996265411377, \"max\": 421.5996265411377}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1621.7219295349814 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] #quality_metric: host=algo-1, epoch=99, train loss <loss>=3.6983048265630547\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] Epoch[100] Batch[0] avg_epoch_loss=3.768619\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=3.768618583679199\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] Epoch[100] Batch[5] avg_epoch_loss=3.646978\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=3.646978497505188\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] Epoch[100] Batch [5]#011Speed: 3073.03 samples/sec#011loss=3.646978\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] Epoch[100] Batch[10] avg_epoch_loss=3.654319\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] #quality_metric: host=algo-1, epoch=100, batch=10 train loss <loss>=3.663127040863037\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] Epoch[100] Batch [10]#011Speed: 2089.43 samples/sec#011loss=3.663127\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] processed a total of 698 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916090.0419223, \"EndTime\": 1680916090.5307434, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 488.16442489624023, \"count\": 1, \"min\": 488.16442489624023, \"max\": 488.16442489624023}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1429.3958181875882 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] #progress_metric: host=algo-1, completed 25.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] #quality_metric: host=algo-1, epoch=100, train loss <loss>=3.6543187444860283\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] Epoch[101] Batch[0] avg_epoch_loss=3.702873\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=3.7028729915618896\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] Epoch[101] Batch[5] avg_epoch_loss=3.609700\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=3.6097000439961753\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] Epoch[101] Batch [5]#011Speed: 2875.33 samples/sec#011loss=3.609700\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] Epoch[101] Batch[10] avg_epoch_loss=3.855821\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] #quality_metric: host=algo-1, epoch=101, batch=10 train loss <loss>=4.151167011260986\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] Epoch[101] Batch [10]#011Speed: 2866.58 samples/sec#011loss=4.151167\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916090.5308485, \"EndTime\": 1680916090.942444, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 410.97474098205566, \"count\": 1, \"min\": 410.97474098205566, \"max\": 410.97474098205566}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1581.0379662856972 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] #quality_metric: host=algo-1, epoch=101, train loss <loss>=3.8558213927529077\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:10 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] Epoch[102] Batch[0] avg_epoch_loss=3.547106\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=3.5471057891845703\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] Epoch[102] Batch[5] avg_epoch_loss=3.738786\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=3.7387861013412476\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] Epoch[102] Batch [5]#011Speed: 2659.48 samples/sec#011loss=3.738786\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] Epoch[102] Batch[10] avg_epoch_loss=3.923552\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] #quality_metric: host=algo-1, epoch=102, batch=10 train loss <loss>=4.145270824432373\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] Epoch[102] Batch [10]#011Speed: 2830.19 samples/sec#011loss=4.145271\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916090.9425466, \"EndTime\": 1680916091.4107995, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 467.5109386444092, \"count\": 1, \"min\": 467.5109386444092, \"max\": 467.5109386444092}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1411.3063135172833 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] #progress_metric: host=algo-1, completed 25.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] #quality_metric: host=algo-1, epoch=102, train loss <loss>=3.9235518845644863\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] Epoch[103] Batch[0] avg_epoch_loss=3.886300\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=3.8863000869750977\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] Epoch[103] Batch[5] avg_epoch_loss=3.896801\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=3.8968010346094766\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] Epoch[103] Batch [5]#011Speed: 3094.41 samples/sec#011loss=3.896801\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] Epoch[103] Batch[10] avg_epoch_loss=3.846340\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=3.7857862949371337\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] Epoch[103] Batch [10]#011Speed: 2694.19 samples/sec#011loss=3.785786\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916091.4108982, \"EndTime\": 1680916091.8182948, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 406.890869140625, \"count\": 1, \"min\": 406.890869140625, \"max\": 406.890869140625}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1636.2953197516567 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] #quality_metric: host=algo-1, epoch=103, train loss <loss>=3.846339789303866\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:11 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] Epoch[104] Batch[0] avg_epoch_loss=3.529156\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=3.529156446456909\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] Epoch[104] Batch[5] avg_epoch_loss=3.753357\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=3.7533567746480307\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] Epoch[104] Batch [5]#011Speed: 3125.53 samples/sec#011loss=3.753357\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] Epoch[104] Batch[10] avg_epoch_loss=3.797845\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] #quality_metric: host=algo-1, epoch=104, batch=10 train loss <loss>=3.8512301445007324\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] Epoch[104] Batch [10]#011Speed: 2641.74 samples/sec#011loss=3.851230\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916091.8183818, \"EndTime\": 1680916092.2748873, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 455.89590072631836, \"count\": 1, \"min\": 455.89590072631836, \"max\": 455.89590072631836}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1460.4649406096044 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] #progress_metric: host=algo-1, completed 26.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] #quality_metric: host=algo-1, epoch=104, train loss <loss>=3.7978446700356225\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] Epoch[105] Batch[0] avg_epoch_loss=3.379313\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=3.3793134689331055\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] Epoch[105] Batch[5] avg_epoch_loss=3.700754\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=3.7007543643315635\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] Epoch[105] Batch [5]#011Speed: 2947.91 samples/sec#011loss=3.700754\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] Epoch[105] Batch[10] avg_epoch_loss=3.666486\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] #quality_metric: host=algo-1, epoch=105, batch=10 train loss <loss>=3.62536416053772\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] Epoch[105] Batch [10]#011Speed: 2774.62 samples/sec#011loss=3.625364\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916092.2749693, \"EndTime\": 1680916092.6870747, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 411.301851272583, \"count\": 1, \"min\": 411.301851272583, \"max\": 411.301851272583}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1652.7763438641955 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] #quality_metric: host=algo-1, epoch=105, train loss <loss>=3.6664860898798164\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] Epoch[106] Batch[0] avg_epoch_loss=3.254960\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=3.25495982170105\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] Epoch[106] Batch[5] avg_epoch_loss=3.619628\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=3.619628151257833\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:12 INFO 140448693614400] Epoch[106] Batch [5]#011Speed: 3102.65 samples/sec#011loss=3.619628\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916092.687158, \"EndTime\": 1680916093.0581126, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 370.1756000518799, \"count\": 1, \"min\": 370.1756000518799, \"max\": 370.1756000518799}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1670.6191744435127 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] #progress_metric: host=algo-1, completed 26.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] #quality_metric: host=algo-1, epoch=106, train loss <loss>=3.689793920516968\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] Epoch[107] Batch[0] avg_epoch_loss=3.919696\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=3.9196956157684326\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] Epoch[107] Batch[5] avg_epoch_loss=3.789899\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=3.7898987929026284\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] Epoch[107] Batch [5]#011Speed: 3096.94 samples/sec#011loss=3.789899\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] Epoch[107] Batch[10] avg_epoch_loss=3.918721\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] #quality_metric: host=algo-1, epoch=107, batch=10 train loss <loss>=4.07330732345581\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] Epoch[107] Batch [10]#011Speed: 2822.78 samples/sec#011loss=4.073307\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916093.0584085, \"EndTime\": 1680916093.4609227, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 401.65162086486816, \"count\": 1, \"min\": 401.65162086486816, \"max\": 401.65162086486816}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1642.6797563253695 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] #quality_metric: host=algo-1, epoch=107, train loss <loss>=3.918720852244984\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] Epoch[108] Batch[0] avg_epoch_loss=4.086676\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=4.086675643920898\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] Epoch[108] Batch[5] avg_epoch_loss=3.878477\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=3.878476699193319\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] Epoch[108] Batch [5]#011Speed: 3163.71 samples/sec#011loss=3.878477\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] Epoch[108] Batch[10] avg_epoch_loss=3.764320\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] #quality_metric: host=algo-1, epoch=108, batch=10 train loss <loss>=3.6273327350616453\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] Epoch[108] Batch [10]#011Speed: 2266.87 samples/sec#011loss=3.627333\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] processed a total of 734 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916093.4610136, \"EndTime\": 1680916093.9588077, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 497.0569610595703, \"count\": 1, \"min\": 497.0569610595703, \"max\": 497.0569610595703}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1476.2819256090118 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] #progress_metric: host=algo-1, completed 27.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] #quality_metric: host=algo-1, epoch=108, train loss <loss>=3.703175882498423\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:13 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] Epoch[109] Batch[0] avg_epoch_loss=3.690066\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=3.690066337585449\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] Epoch[109] Batch[5] avg_epoch_loss=3.686601\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=3.6866008838017783\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] Epoch[109] Batch [5]#011Speed: 3048.45 samples/sec#011loss=3.686601\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916093.958901, \"EndTime\": 1680916094.3407931, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 381.21962547302246, \"count\": 1, \"min\": 381.21962547302246, \"max\": 381.21962547302246}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1625.328354057745 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] #quality_metric: host=algo-1, epoch=109, train loss <loss>=3.645159363746643\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] Epoch[110] Batch[0] avg_epoch_loss=3.729037\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=3.729036569595337\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] Epoch[110] Batch[5] avg_epoch_loss=3.777301\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=3.7773013909657798\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] Epoch[110] Batch [5]#011Speed: 2916.12 samples/sec#011loss=3.777301\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] Epoch[110] Batch[10] avg_epoch_loss=3.629820\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] #quality_metric: host=algo-1, epoch=110, batch=10 train loss <loss>=3.452842664718628\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] Epoch[110] Batch [10]#011Speed: 2700.23 samples/sec#011loss=3.452843\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916094.3409836, \"EndTime\": 1680916094.7539623, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 412.2593402862549, \"count\": 1, \"min\": 412.2593402862549, \"max\": 412.2593402862549}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1622.2095410100865 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] #progress_metric: host=algo-1, completed 27.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] #quality_metric: host=algo-1, epoch=110, train loss <loss>=3.629820151762529\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] Epoch[111] Batch[0] avg_epoch_loss=3.634512\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:14 INFO 140448693614400] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=3.634511947631836\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] Epoch[111] Batch[5] avg_epoch_loss=3.701545\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=3.7015453974405923\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] Epoch[111] Batch [5]#011Speed: 3080.44 samples/sec#011loss=3.701545\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] Epoch[111] Batch[10] avg_epoch_loss=3.593165\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] #quality_metric: host=algo-1, epoch=111, batch=10 train loss <loss>=3.4631093978881835\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] Epoch[111] Batch [10]#011Speed: 2646.14 samples/sec#011loss=3.463109\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916094.754057, \"EndTime\": 1680916095.1574557, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 402.83775329589844, \"count\": 1, \"min\": 402.83775329589844, \"max\": 402.83775329589844}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1645.2347888954691 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] #quality_metric: host=algo-1, epoch=111, train loss <loss>=3.593165397644043\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_eacc2e4a-a711-437e-b586-5055a04de093-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916095.1575599, \"EndTime\": 1680916095.1725106, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 14.442920684814453, \"count\": 1, \"min\": 14.442920684814453, \"max\": 14.442920684814453}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] Epoch[112] Batch[0] avg_epoch_loss=3.487046\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=3.487046480178833\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] Epoch[112] Batch[5] avg_epoch_loss=3.740538\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=3.740538398424784\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] Epoch[112] Batch [5]#011Speed: 2769.65 samples/sec#011loss=3.740538\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] Epoch[112] Batch[10] avg_epoch_loss=3.761000\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] #quality_metric: host=algo-1, epoch=112, batch=10 train loss <loss>=3.7855547428131104\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] Epoch[112] Batch [10]#011Speed: 2904.98 samples/sec#011loss=3.785555\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916095.172593, \"EndTime\": 1680916095.6048484, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 432.19542503356934, \"count\": 1, \"min\": 432.19542503356934, \"max\": 432.19542503356934}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1540.51492146337 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] #progress_metric: host=algo-1, completed 28.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] #quality_metric: host=algo-1, epoch=112, train loss <loss>=3.7610003731467505\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] Epoch[113] Batch[0] avg_epoch_loss=4.147922\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=4.147921562194824\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] Epoch[113] Batch[5] avg_epoch_loss=3.895721\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=3.895720958709717\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:15 INFO 140448693614400] Epoch[113] Batch [5]#011Speed: 2938.90 samples/sec#011loss=3.895721\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] Epoch[113] Batch[10] avg_epoch_loss=3.937449\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] #quality_metric: host=algo-1, epoch=113, batch=10 train loss <loss>=3.9875229835510253\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] Epoch[113] Batch [10]#011Speed: 2338.44 samples/sec#011loss=3.987523\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916095.604938, \"EndTime\": 1680916096.0463789, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 440.8717155456543, \"count\": 1, \"min\": 440.8717155456543, \"max\": 440.8717155456543}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1503.3689018061034 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] #quality_metric: host=algo-1, epoch=113, train loss <loss>=3.9374491518194024\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] Epoch[114] Batch[0] avg_epoch_loss=3.983311\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=3.9833109378814697\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] Epoch[114] Batch[5] avg_epoch_loss=3.922666\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=3.9226659536361694\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] Epoch[114] Batch [5]#011Speed: 2860.19 samples/sec#011loss=3.922666\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] Epoch[114] Batch[10] avg_epoch_loss=3.732186\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] #quality_metric: host=algo-1, epoch=114, batch=10 train loss <loss>=3.503610706329346\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] Epoch[114] Batch [10]#011Speed: 2844.05 samples/sec#011loss=3.503611\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916096.0464718, \"EndTime\": 1680916096.4565365, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 409.4557762145996, \"count\": 1, \"min\": 409.4557762145996, \"max\": 409.4557762145996}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1626.0257017873203 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] #progress_metric: host=algo-1, completed 28.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] #quality_metric: host=algo-1, epoch=114, train loss <loss>=3.732186295769431\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] Epoch[115] Batch[0] avg_epoch_loss=3.779341\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=3.779341459274292\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] Epoch[115] Batch[5] avg_epoch_loss=3.844428\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=3.844428022702535\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] Epoch[115] Batch [5]#011Speed: 2834.90 samples/sec#011loss=3.844428\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] Epoch[115] Batch[10] avg_epoch_loss=3.848495\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] #quality_metric: host=algo-1, epoch=115, batch=10 train loss <loss>=3.853375196456909\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] Epoch[115] Batch [10]#011Speed: 2624.89 samples/sec#011loss=3.853375\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916096.4566247, \"EndTime\": 1680916096.9254215, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 468.2135581970215, \"count\": 1, \"min\": 468.2135581970215, \"max\": 468.2135581970215}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1430.5433520479612 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] #quality_metric: host=algo-1, epoch=115, train loss <loss>=3.8484949198636142\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:16 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] Epoch[116] Batch[0] avg_epoch_loss=3.689634\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=3.689634323120117\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] Epoch[116] Batch[5] avg_epoch_loss=3.637284\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=3.637284437815348\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] Epoch[116] Batch [5]#011Speed: 3057.55 samples/sec#011loss=3.637284\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] Epoch[116] Batch[10] avg_epoch_loss=3.722376\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] #quality_metric: host=algo-1, epoch=116, batch=10 train loss <loss>=3.8244855880737303\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] Epoch[116] Batch [10]#011Speed: 2699.57 samples/sec#011loss=3.824486\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916096.9255166, \"EndTime\": 1680916097.3360503, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 409.90424156188965, \"count\": 1, \"min\": 409.90424156188965, \"max\": 409.90424156188965}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1619.2990462767698 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] #progress_metric: host=algo-1, completed 29.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] #quality_metric: host=algo-1, epoch=116, train loss <loss>=3.7223758697509766\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] Epoch[117] Batch[0] avg_epoch_loss=3.780682\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=3.780681848526001\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] Epoch[117] Batch[5] avg_epoch_loss=3.693551\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=3.6935506661732993\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] Epoch[117] Batch [5]#011Speed: 3131.21 samples/sec#011loss=3.693551\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] Epoch[117] Batch[10] avg_epoch_loss=3.612184\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] #quality_metric: host=algo-1, epoch=117, batch=10 train loss <loss>=3.5145437717437744\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] Epoch[117] Batch [10]#011Speed: 2630.97 samples/sec#011loss=3.514544\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916097.3361557, \"EndTime\": 1680916097.770236, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 433.55417251586914, \"count\": 1, \"min\": 433.55417251586914, \"max\": 433.55417251586914}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1567.9132562495224 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] #quality_metric: host=algo-1, epoch=117, train loss <loss>=3.6121838959780606\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] Epoch[118] Batch[0] avg_epoch_loss=3.555115\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:17 INFO 140448693614400] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=3.555114984512329\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] Epoch[118] Batch[5] avg_epoch_loss=3.762315\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=3.7623148759206138\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] Epoch[118] Batch [5]#011Speed: 2548.37 samples/sec#011loss=3.762315\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] Epoch[118] Batch[10] avg_epoch_loss=3.708096\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] #quality_metric: host=algo-1, epoch=118, batch=10 train loss <loss>=3.643032741546631\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] Epoch[118] Batch [10]#011Speed: 2489.88 samples/sec#011loss=3.643033\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] processed a total of 692 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916097.7703342, \"EndTime\": 1680916098.2114127, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 440.4778480529785, \"count\": 1, \"min\": 440.4778480529785, \"max\": 440.4778480529785}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1570.565459293498 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] #progress_metric: host=algo-1, completed 29.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] #quality_metric: host=algo-1, epoch=118, train loss <loss>=3.7080957239324395\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] Epoch[119] Batch[0] avg_epoch_loss=3.955833\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=3.9558327198028564\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] Epoch[119] Batch[5] avg_epoch_loss=3.568388\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=3.568387826283773\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] Epoch[119] Batch [5]#011Speed: 3157.32 samples/sec#011loss=3.568388\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] Epoch[119] Batch[10] avg_epoch_loss=3.585520\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] #quality_metric: host=algo-1, epoch=119, batch=10 train loss <loss>=3.6060781955718992\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] Epoch[119] Batch [10]#011Speed: 2601.01 samples/sec#011loss=3.606078\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916098.2114992, \"EndTime\": 1680916098.6724236, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 460.3228569030762, \"count\": 1, \"min\": 460.3228569030762, \"max\": 460.3228569030762}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1428.9593263234633 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] #quality_metric: host=algo-1, epoch=119, train loss <loss>=3.5855198123238305\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_04811d17-bc91-4472-9b80-306529dddff7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916098.6725297, \"EndTime\": 1680916098.6856248, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.314558029174805, \"count\": 1, \"min\": 12.314558029174805, \"max\": 12.314558029174805}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] Epoch[120] Batch[0] avg_epoch_loss=3.519435\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=3.519434928894043\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] Epoch[120] Batch[5] avg_epoch_loss=3.673821\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=3.673821051915487\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:18 INFO 140448693614400] Epoch[120] Batch [5]#011Speed: 3089.84 samples/sec#011loss=3.673821\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] Epoch[120] Batch[10] avg_epoch_loss=3.641888\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] #quality_metric: host=algo-1, epoch=120, batch=10 train loss <loss>=3.6035675525665285\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] Epoch[120] Batch [10]#011Speed: 2466.34 samples/sec#011loss=3.603568\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] processed a total of 689 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916098.685706, \"EndTime\": 1680916099.0996616, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 413.8922691345215, \"count\": 1, \"min\": 413.8922691345215, \"max\": 413.8922691345215}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1664.168649166613 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] #progress_metric: host=algo-1, completed 30.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] #quality_metric: host=algo-1, epoch=120, train loss <loss>=3.6418876431205054\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] Epoch[121] Batch[0] avg_epoch_loss=3.505802\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=3.5058019161224365\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] Epoch[121] Batch[5] avg_epoch_loss=3.564528\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=3.5645277897516885\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] Epoch[121] Batch [5]#011Speed: 3099.45 samples/sec#011loss=3.564528\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] Epoch[121] Batch[10] avg_epoch_loss=3.616411\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] #quality_metric: host=algo-1, epoch=121, batch=10 train loss <loss>=3.678669786453247\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] Epoch[121] Batch [10]#011Speed: 2800.08 samples/sec#011loss=3.678670\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916099.0997531, \"EndTime\": 1680916099.5569546, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 456.6214084625244, \"count\": 1, \"min\": 456.6214084625244, \"max\": 456.6214084625244}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1446.9895876935411 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] #quality_metric: host=algo-1, epoch=121, train loss <loss>=3.6164105155251245\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] Epoch[122] Batch[0] avg_epoch_loss=3.401085\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=3.4010846614837646\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] Epoch[122] Batch[5] avg_epoch_loss=3.656070\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=3.6560699542363486\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:19 INFO 140448693614400] Epoch[122] Batch [5]#011Speed: 3105.45 samples/sec#011loss=3.656070\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] Epoch[122] Batch[10] avg_epoch_loss=3.631932\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] #quality_metric: host=algo-1, epoch=122, batch=10 train loss <loss>=3.602965784072876\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] Epoch[122] Batch [10]#011Speed: 2486.09 samples/sec#011loss=3.602966\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] processed a total of 721 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916099.5570757, \"EndTime\": 1680916100.040172, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 482.01608657836914, \"count\": 1, \"min\": 482.01608657836914, \"max\": 482.01608657836914}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1495.3909278456551 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] #progress_metric: host=algo-1, completed 30.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] #quality_metric: host=algo-1, epoch=122, train loss <loss>=3.4505613346894584\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_8ca3e62e-2033-4f29-af2d-11faf2e233da-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916100.0402627, \"EndTime\": 1680916100.054415, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.636350631713867, \"count\": 1, \"min\": 13.636350631713867, \"max\": 13.636350631713867}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] Epoch[123] Batch[0] avg_epoch_loss=3.697126\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=3.6971256732940674\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] Epoch[123] Batch[5] avg_epoch_loss=3.717860\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=3.7178595463434854\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] Epoch[123] Batch [5]#011Speed: 2330.93 samples/sec#011loss=3.717860\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] Epoch[123] Batch[10] avg_epoch_loss=3.702800\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] #quality_metric: host=algo-1, epoch=123, batch=10 train loss <loss>=3.684728574752808\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] Epoch[123] Batch [10]#011Speed: 2711.75 samples/sec#011loss=3.684729\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916100.054493, \"EndTime\": 1680916100.5312572, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 476.7038822174072, \"count\": 1, \"min\": 476.7038822174072, \"max\": 476.7038822174072}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1423.9676319676319 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] #quality_metric: host=algo-1, epoch=123, train loss <loss>=3.702800013802268\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] Epoch[124] Batch[0] avg_epoch_loss=3.612089\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=3.612088680267334\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] Epoch[124] Batch[5] avg_epoch_loss=3.643101\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=3.6431008179982505\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] Epoch[124] Batch [5]#011Speed: 2625.55 samples/sec#011loss=3.643101\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] Epoch[124] Batch[10] avg_epoch_loss=3.627974\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] #quality_metric: host=algo-1, epoch=124, batch=10 train loss <loss>=3.6098211288452147\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] Epoch[124] Batch [10]#011Speed: 2199.74 samples/sec#011loss=3.609821\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916100.5313513, \"EndTime\": 1680916100.9818635, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 449.9497413635254, \"count\": 1, \"min\": 449.9497413635254, \"max\": 449.9497413635254}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1472.9601449217787 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] #progress_metric: host=algo-1, completed 31.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] #quality_metric: host=algo-1, epoch=124, train loss <loss>=3.6279736865650523\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:20 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] Epoch[125] Batch[0] avg_epoch_loss=3.810958\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=3.810957908630371\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] Epoch[125] Batch[5] avg_epoch_loss=3.768985\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=3.7689850330352783\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] Epoch[125] Batch [5]#011Speed: 3140.15 samples/sec#011loss=3.768985\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] Epoch[125] Batch[10] avg_epoch_loss=3.669787\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] #quality_metric: host=algo-1, epoch=125, batch=10 train loss <loss>=3.550749588012695\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] Epoch[125] Batch [10]#011Speed: 2133.15 samples/sec#011loss=3.550750\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] processed a total of 748 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916100.9819767, \"EndTime\": 1680916101.4388583, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 456.22897148132324, \"count\": 1, \"min\": 456.22897148132324, \"max\": 456.22897148132324}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1638.7371816769619 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] #quality_metric: host=algo-1, epoch=125, train loss <loss>=3.686500291029612\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] Epoch[126] Batch[0] avg_epoch_loss=3.623669\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=3.623669385910034\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] Epoch[126] Batch[5] avg_epoch_loss=3.507607\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=3.5076073010762534\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] Epoch[126] Batch [5]#011Speed: 2541.62 samples/sec#011loss=3.507607\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] Epoch[126] Batch[10] avg_epoch_loss=3.595533\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] #quality_metric: host=algo-1, epoch=126, batch=10 train loss <loss>=3.7010435104370116\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] Epoch[126] Batch [10]#011Speed: 2639.24 samples/sec#011loss=3.701044\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] processed a total of 696 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916101.4390302, \"EndTime\": 1680916101.8714519, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 431.66112899780273, \"count\": 1, \"min\": 431.66112899780273, \"max\": 431.66112899780273}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1611.8310579582912 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] #progress_metric: host=algo-1, completed 31.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] #quality_metric: host=algo-1, epoch=126, train loss <loss>=3.595532850785689\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:21 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] Epoch[127] Batch[0] avg_epoch_loss=3.561786\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=3.561786413192749\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] Epoch[127] Batch[5] avg_epoch_loss=3.661250\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=3.6612497568130493\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] Epoch[127] Batch [5]#011Speed: 2500.55 samples/sec#011loss=3.661250\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916101.8715518, \"EndTime\": 1680916102.2708597, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 398.6241817474365, \"count\": 1, \"min\": 398.6241817474365, \"max\": 398.6241817474365}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1604.9885351750397 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] #quality_metric: host=algo-1, epoch=127, train loss <loss>=3.6388850450515746\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] Epoch[128] Batch[0] avg_epoch_loss=3.580556\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=3.5805561542510986\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] Epoch[128] Batch[5] avg_epoch_loss=3.695461\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=3.6954610347747803\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] Epoch[128] Batch [5]#011Speed: 3026.49 samples/sec#011loss=3.695461\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] Epoch[128] Batch[10] avg_epoch_loss=3.795998\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] #quality_metric: host=algo-1, epoch=128, batch=10 train loss <loss>=3.916642665863037\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] Epoch[128] Batch [10]#011Speed: 2952.77 samples/sec#011loss=3.916643\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916102.2709498, \"EndTime\": 1680916102.6871874, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 415.70353507995605, \"count\": 1, \"min\": 415.70353507995605, \"max\": 415.70353507995605}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1570.2888345242402 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] #progress_metric: host=algo-1, completed 32.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] #quality_metric: host=algo-1, epoch=128, train loss <loss>=3.795998139814897\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] Epoch[129] Batch[0] avg_epoch_loss=3.533045\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=3.5330450534820557\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] Epoch[129] Batch[5] avg_epoch_loss=3.612299\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=3.6122989654541016\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:22 INFO 140448693614400] Epoch[129] Batch [5]#011Speed: 2623.79 samples/sec#011loss=3.612299\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] Epoch[129] Batch[10] avg_epoch_loss=3.715678\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] #quality_metric: host=algo-1, epoch=129, batch=10 train loss <loss>=3.8397320747375487\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] Epoch[129] Batch [10]#011Speed: 2631.98 samples/sec#011loss=3.839732\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] processed a total of 706 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916102.6872754, \"EndTime\": 1680916103.142661, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 454.770565032959, \"count\": 1, \"min\": 454.770565032959, \"max\": 454.770565032959}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1551.9787840893248 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] #quality_metric: host=algo-1, epoch=129, train loss <loss>=3.844393014907837\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] Epoch[130] Batch[0] avg_epoch_loss=3.854120\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=3.8541197776794434\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] Epoch[130] Batch[5] avg_epoch_loss=4.108973\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=4.108972628911336\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] Epoch[130] Batch [5]#011Speed: 2940.37 samples/sec#011loss=4.108973\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] Epoch[130] Batch[10] avg_epoch_loss=3.991836\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] #quality_metric: host=algo-1, epoch=130, batch=10 train loss <loss>=3.8512712955474853\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] Epoch[130] Batch [10]#011Speed: 2617.36 samples/sec#011loss=3.851271\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916103.1427474, \"EndTime\": 1680916103.5552185, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 411.851167678833, \"count\": 1, \"min\": 411.851167678833, \"max\": 411.851167678833}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1655.3362117106092 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] #progress_metric: host=algo-1, completed 32.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] #quality_metric: host=algo-1, epoch=130, train loss <loss>=3.991835659200495\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] Epoch[131] Batch[0] avg_epoch_loss=3.892610\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=3.8926098346710205\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] Epoch[131] Batch[5] avg_epoch_loss=3.970254\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=3.970253666241964\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:23 INFO 140448693614400] Epoch[131] Batch [5]#011Speed: 3152.29 samples/sec#011loss=3.970254\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] Epoch[131] Batch[10] avg_epoch_loss=3.997431\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] #quality_metric: host=algo-1, epoch=131, batch=10 train loss <loss>=4.030043268203736\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] Epoch[131] Batch [10]#011Speed: 2575.51 samples/sec#011loss=4.030043\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] processed a total of 692 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916103.555324, \"EndTime\": 1680916104.0137355, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 457.7937126159668, \"count\": 1, \"min\": 457.7937126159668, \"max\": 457.7937126159668}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1511.106003849536 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] #quality_metric: host=algo-1, epoch=131, train loss <loss>=3.997430758042769\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] Epoch[132] Batch[0] avg_epoch_loss=4.027689\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=4.027689456939697\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] Epoch[132] Batch[5] avg_epoch_loss=3.842249\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=3.8422489166259766\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] Epoch[132] Batch [5]#011Speed: 2761.37 samples/sec#011loss=3.842249\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] Epoch[132] Batch[10] avg_epoch_loss=3.797354\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] #quality_metric: host=algo-1, epoch=132, batch=10 train loss <loss>=3.7434802532196043\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] Epoch[132] Batch [10]#011Speed: 2668.63 samples/sec#011loss=3.743480\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916104.0138447, \"EndTime\": 1680916104.4268312, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 412.45055198669434, \"count\": 1, \"min\": 412.45055198669434, \"max\": 412.45055198669434}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1643.2682275685365 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] #progress_metric: host=algo-1, completed 33.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] #quality_metric: host=algo-1, epoch=132, train loss <loss>=3.79735406962308\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] Epoch[133] Batch[0] avg_epoch_loss=4.031253\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=4.031253337860107\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] Epoch[133] Batch[5] avg_epoch_loss=3.765544\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=3.7655438979466758\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] Epoch[133] Batch [5]#011Speed: 2759.76 samples/sec#011loss=3.765544\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] Epoch[133] Batch[10] avg_epoch_loss=3.695324\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] #quality_metric: host=algo-1, epoch=133, batch=10 train loss <loss>=3.611059808731079\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] Epoch[133] Batch [10]#011Speed: 2514.18 samples/sec#011loss=3.611060\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] processed a total of 693 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916104.42693, \"EndTime\": 1680916104.8454297, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 417.9267883300781, \"count\": 1, \"min\": 417.9267883300781, \"max\": 417.9267883300781}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1657.4769422368406 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] #quality_metric: host=algo-1, epoch=133, train loss <loss>=3.695323857394132\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:24 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] Epoch[134] Batch[0] avg_epoch_loss=3.720929\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=3.720928907394409\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] Epoch[134] Batch[5] avg_epoch_loss=3.668790\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=3.668790062268575\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] Epoch[134] Batch [5]#011Speed: 3106.90 samples/sec#011loss=3.668790\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916104.8455646, \"EndTime\": 1680916105.2386806, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 392.5008773803711, \"count\": 1, \"min\": 392.5008773803711, \"max\": 392.5008773803711}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1614.7307987279812 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] #progress_metric: host=algo-1, completed 33.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] #quality_metric: host=algo-1, epoch=134, train loss <loss>=3.724310779571533\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] Epoch[135] Batch[0] avg_epoch_loss=3.689093\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=3.6890928745269775\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] Epoch[135] Batch[5] avg_epoch_loss=3.693395\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=3.693394939104716\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] Epoch[135] Batch [5]#011Speed: 2842.40 samples/sec#011loss=3.693395\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916105.23877, \"EndTime\": 1680916105.6716137, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 432.15465545654297, \"count\": 1, \"min\": 432.15465545654297, \"max\": 432.15465545654297}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1480.3722501057466 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] #quality_metric: host=algo-1, epoch=135, train loss <loss>=3.7088542938232423\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] Epoch[136] Batch[0] avg_epoch_loss=3.538159\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=3.538158893585205\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] Epoch[136] Batch[5] avg_epoch_loss=3.618666\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=3.618666330973307\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:25 INFO 140448693614400] Epoch[136] Batch [5]#011Speed: 3025.50 samples/sec#011loss=3.618666\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] Epoch[136] Batch[10] avg_epoch_loss=3.551847\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] #quality_metric: host=algo-1, epoch=136, batch=10 train loss <loss>=3.4716639041900637\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] Epoch[136] Batch [10]#011Speed: 2304.65 samples/sec#011loss=3.471664\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] processed a total of 744 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916105.6717358, \"EndTime\": 1680916106.1222281, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 449.36513900756836, \"count\": 1, \"min\": 449.36513900756836, \"max\": 449.36513900756836}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1655.1545466117877 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] #progress_metric: host=algo-1, completed 34.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] #quality_metric: host=algo-1, epoch=136, train loss <loss>=3.4706650773684182\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] Epoch[137] Batch[0] avg_epoch_loss=3.369827\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=3.3698272705078125\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] Epoch[137] Batch[5] avg_epoch_loss=3.444883\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=3.4448829094568887\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] Epoch[137] Batch [5]#011Speed: 2830.64 samples/sec#011loss=3.444883\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] Epoch[137] Batch[10] avg_epoch_loss=3.554330\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] #quality_metric: host=algo-1, epoch=137, batch=10 train loss <loss>=3.68566689491272\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] Epoch[137] Batch [10]#011Speed: 2829.19 samples/sec#011loss=3.685667\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916106.1223247, \"EndTime\": 1680916106.5723665, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 449.53441619873047, \"count\": 1, \"min\": 449.53441619873047, \"max\": 449.53441619873047}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1489.9165118085398 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] #quality_metric: host=algo-1, epoch=137, train loss <loss>=3.5543301755731758\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] Epoch[138] Batch[0] avg_epoch_loss=3.609037\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=3.609037399291992\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] Epoch[138] Batch[5] avg_epoch_loss=3.620532\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=3.620532194773356\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:26 INFO 140448693614400] Epoch[138] Batch [5]#011Speed: 2822.02 samples/sec#011loss=3.620532\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] Epoch[138] Batch[10] avg_epoch_loss=3.461217\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] #quality_metric: host=algo-1, epoch=138, batch=10 train loss <loss>=3.2700386524200438\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] Epoch[138] Batch [10]#011Speed: 2570.25 samples/sec#011loss=3.270039\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916106.5724754, \"EndTime\": 1680916107.0275128, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 454.51974868774414, \"count\": 1, \"min\": 454.51974868774414, \"max\": 454.51974868774414}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1504.457378747897 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] #progress_metric: host=algo-1, completed 34.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] #quality_metric: host=algo-1, epoch=138, train loss <loss>=3.4612169482491235\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] Epoch[139] Batch[0] avg_epoch_loss=3.365320\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=3.3653199672698975\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] Epoch[139] Batch[5] avg_epoch_loss=3.471823\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=3.47182289759318\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] Epoch[139] Batch [5]#011Speed: 3100.52 samples/sec#011loss=3.471823\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] Epoch[139] Batch[10] avg_epoch_loss=3.525276\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] #quality_metric: host=algo-1, epoch=139, batch=10 train loss <loss>=3.5894190311431884\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] Epoch[139] Batch [10]#011Speed: 2659.95 samples/sec#011loss=3.589419\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916107.027602, \"EndTime\": 1680916107.485794, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 457.64946937561035, \"count\": 1, \"min\": 457.64946937561035, \"max\": 457.64946937561035}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1434.8654844392004 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] #quality_metric: host=algo-1, epoch=139, train loss <loss>=3.5252756855704566\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] Epoch[140] Batch[0] avg_epoch_loss=3.613154\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=3.6131536960601807\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] Epoch[140] Batch[5] avg_epoch_loss=3.624077\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=3.6240766048431396\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] Epoch[140] Batch [5]#011Speed: 2963.78 samples/sec#011loss=3.624077\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] Epoch[140] Batch[10] avg_epoch_loss=3.614260\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] #quality_metric: host=algo-1, epoch=140, batch=10 train loss <loss>=3.602479600906372\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] Epoch[140] Batch [10]#011Speed: 2602.56 samples/sec#011loss=3.602480\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] processed a total of 715 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916107.4859858, \"EndTime\": 1680916107.9268255, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 440.096378326416, \"count\": 1, \"min\": 440.096378326416, \"max\": 440.096378326416}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1624.1109469570604 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] #progress_metric: host=algo-1, completed 35.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] #quality_metric: host=algo-1, epoch=140, train loss <loss>=3.390558938185374\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:27 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/state_95414d64-ec73-4a3b-ba01-2debaa8a27a0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916107.9269238, \"EndTime\": 1680916107.9379222, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 10.321617126464844, \"count\": 1, \"min\": 10.321617126464844, \"max\": 10.321617126464844}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] Epoch[141] Batch[0] avg_epoch_loss=3.742129\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=3.74212908744812\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] Epoch[141] Batch[5] avg_epoch_loss=3.968909\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=3.9689090251922607\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] Epoch[141] Batch [5]#011Speed: 3060.94 samples/sec#011loss=3.968909\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] Epoch[141] Batch[10] avg_epoch_loss=3.942114\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] #quality_metric: host=algo-1, epoch=141, batch=10 train loss <loss>=3.909960222244263\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] Epoch[141] Batch [10]#011Speed: 2640.94 samples/sec#011loss=3.909960\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916107.9380035, \"EndTime\": 1680916108.3433301, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 405.26795387268066, \"count\": 1, \"min\": 405.26795387268066, \"max\": 405.26795387268066}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1608.0723368648341 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] #quality_metric: host=algo-1, epoch=141, train loss <loss>=3.9421141147613525\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] Epoch[142] Batch[0] avg_epoch_loss=3.705210\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=3.705209970474243\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] Epoch[142] Batch[5] avg_epoch_loss=3.642852\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=3.6428515911102295\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] Epoch[142] Batch [5]#011Speed: 3168.52 samples/sec#011loss=3.642852\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] Epoch[142] Batch[10] avg_epoch_loss=3.670154\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] #quality_metric: host=algo-1, epoch=142, batch=10 train loss <loss>=3.702917289733887\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] Epoch[142] Batch [10]#011Speed: 2564.30 samples/sec#011loss=3.702917\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] processed a total of 695 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916108.3434315, \"EndTime\": 1680916108.7967825, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 452.77976989746094, \"count\": 1, \"min\": 452.77976989746094, \"max\": 452.77976989746094}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1534.3598467662048 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] #progress_metric: host=algo-1, completed 35.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] #quality_metric: host=algo-1, epoch=142, train loss <loss>=3.67015418139371\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:28 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] Epoch[143] Batch[0] avg_epoch_loss=3.771211\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=3.7712111473083496\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] Epoch[143] Batch[5] avg_epoch_loss=3.552549\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=3.5525487263997397\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] Epoch[143] Batch [5]#011Speed: 3121.62 samples/sec#011loss=3.552549\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916108.796885, \"EndTime\": 1680916109.217517, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 419.9938774108887, \"count\": 1, \"min\": 419.9938774108887, \"max\": 419.9938774108887}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1523.3569808374066 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] #quality_metric: host=algo-1, epoch=143, train loss <loss>=3.5930999755859374\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] Epoch[144] Batch[0] avg_epoch_loss=3.198862\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=3.198861598968506\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] Epoch[144] Batch[5] avg_epoch_loss=3.510992\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=3.5109920501708984\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] Epoch[144] Batch [5]#011Speed: 3094.05 samples/sec#011loss=3.510992\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] Epoch[144] Batch[10] avg_epoch_loss=3.499537\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] #quality_metric: host=algo-1, epoch=144, batch=10 train loss <loss>=3.4857904434204103\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] Epoch[144] Batch [10]#011Speed: 2507.01 samples/sec#011loss=3.485790\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] processed a total of 701 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916109.2176058, \"EndTime\": 1680916109.6806934, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 462.5892639160156, \"count\": 1, \"min\": 462.5892639160156, \"max\": 462.5892639160156}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1514.9381464440105 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] #progress_metric: host=algo-1, completed 36.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] #quality_metric: host=algo-1, epoch=144, train loss <loss>=3.499536774375222\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] Epoch[145] Batch[0] avg_epoch_loss=3.679661\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:29 INFO 140448693614400] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=3.6796605587005615\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] Epoch[145] Batch[5] avg_epoch_loss=3.958543\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=3.9585432211558023\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] Epoch[145] Batch [5]#011Speed: 2997.26 samples/sec#011loss=3.958543\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] Epoch[145] Batch[10] avg_epoch_loss=3.988113\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] #quality_metric: host=algo-1, epoch=145, batch=10 train loss <loss>=4.023597478866577\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] Epoch[145] Batch [10]#011Speed: 2602.88 samples/sec#011loss=4.023597\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] processed a total of 694 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916109.680789, \"EndTime\": 1680916110.1420674, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 460.8142375946045, \"count\": 1, \"min\": 460.8142375946045, \"max\": 460.8142375946045}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1505.587401848489 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] #progress_metric: host=algo-1, completed 36.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] #quality_metric: host=algo-1, epoch=145, train loss <loss>=3.9881133382970635\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] Epoch[146] Batch[0] avg_epoch_loss=3.549092\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=3.5490920543670654\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] Epoch[146] Batch[5] avg_epoch_loss=3.670799\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=3.6707985401153564\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] Epoch[146] Batch [5]#011Speed: 2679.43 samples/sec#011loss=3.670799\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] Epoch[146] Batch[10] avg_epoch_loss=3.683227\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] #quality_metric: host=algo-1, epoch=146, batch=10 train loss <loss>=3.6981412887573244\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] Epoch[146] Batch [10]#011Speed: 2111.08 samples/sec#011loss=3.698141\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] processed a total of 716 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916110.1421616, \"EndTime\": 1680916110.6474166, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 504.7447681427002, \"count\": 1, \"min\": 504.7447681427002, \"max\": 504.7447681427002}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1418.1381464651163 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] #progress_metric: host=algo-1, completed 36.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] #quality_metric: host=algo-1, epoch=146, train loss <loss>=3.5191835860411325\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] Epoch[147] Batch[0] avg_epoch_loss=3.803279\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=3.803278923034668\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] Epoch[147] Batch[5] avg_epoch_loss=3.633226\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=3.6332262754440308\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:30 INFO 140448693614400] Epoch[147] Batch [5]#011Speed: 2686.69 samples/sec#011loss=3.633226\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] Epoch[147] Batch[10] avg_epoch_loss=3.606427\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] #quality_metric: host=algo-1, epoch=147, batch=10 train loss <loss>=3.574266862869263\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] Epoch[147] Batch [10]#011Speed: 2708.61 samples/sec#011loss=3.574267\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916110.6475136, \"EndTime\": 1680916111.069244, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 420.96400260925293, \"count\": 1, \"min\": 420.96400260925293, \"max\": 420.96400260925293}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1626.6785639065222 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] #quality_metric: host=algo-1, epoch=147, train loss <loss>=3.6064265424555\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] Epoch[148] Batch[0] avg_epoch_loss=3.915531\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=3.9155309200286865\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] Epoch[148] Batch[5] avg_epoch_loss=3.748871\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=3.748870531717936\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] Epoch[148] Batch [5]#011Speed: 3132.71 samples/sec#011loss=3.748871\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] Epoch[148] Batch[10] avg_epoch_loss=3.773258\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] #quality_metric: host=algo-1, epoch=148, batch=10 train loss <loss>=3.8025228500366213\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] Epoch[148] Batch [10]#011Speed: 2934.36 samples/sec#011loss=3.802523\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916111.0693378, \"EndTime\": 1680916111.4560466, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 386.0001564025879, \"count\": 1, \"min\": 386.0001564025879, \"max\": 386.0001564025879}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1685.7416908832622 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] #progress_metric: host=algo-1, completed 37.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] #quality_metric: host=algo-1, epoch=148, train loss <loss>=3.77325794913552\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] Epoch[149] Batch[0] avg_epoch_loss=4.805474\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=4.805474281311035\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] Epoch[149] Batch[5] avg_epoch_loss=4.087132\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=4.087132215499878\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] Epoch[149] Batch [5]#011Speed: 3146.38 samples/sec#011loss=4.087132\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] Epoch[149] Batch[10] avg_epoch_loss=3.854638\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] #quality_metric: host=algo-1, epoch=149, batch=10 train loss <loss>=3.575645399093628\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] Epoch[149] Batch [10]#011Speed: 2541.99 samples/sec#011loss=3.575645\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916111.456183, \"EndTime\": 1680916111.9191847, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 462.3851776123047, \"count\": 1, \"min\": 462.3851776123047, \"max\": 462.3851776123047}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1437.8182702954598 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] #quality_metric: host=algo-1, epoch=149, train loss <loss>=3.8546382080424917\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:31 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] Epoch[150] Batch[0] avg_epoch_loss=3.780309\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=3.7803094387054443\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] Epoch[150] Batch[5] avg_epoch_loss=3.757250\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=3.757250348726908\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] Epoch[150] Batch [5]#011Speed: 3054.44 samples/sec#011loss=3.757250\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] Epoch[150] Batch[10] avg_epoch_loss=3.782614\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] #quality_metric: host=algo-1, epoch=150, batch=10 train loss <loss>=3.8130502700805664\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] Epoch[150] Batch [10]#011Speed: 2866.09 samples/sec#011loss=3.813050\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916111.9192646, \"EndTime\": 1680916112.3109882, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 391.2789821624756, \"count\": 1, \"min\": 391.2789821624756, \"max\": 391.2789821624756}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1709.1234867712976 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] #progress_metric: host=algo-1, completed 37.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] #quality_metric: host=algo-1, epoch=150, train loss <loss>=3.7826139493422075\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] Epoch[151] Batch[0] avg_epoch_loss=3.768348\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=3.768348217010498\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] Epoch[151] Batch[5] avg_epoch_loss=3.690420\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=3.690420150756836\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] Epoch[151] Batch [5]#011Speed: 2925.39 samples/sec#011loss=3.690420\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] Epoch[151] Batch[10] avg_epoch_loss=3.740333\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] #quality_metric: host=algo-1, epoch=151, batch=10 train loss <loss>=3.8002288818359373\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] Epoch[151] Batch [10]#011Speed: 2733.60 samples/sec#011loss=3.800229\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916112.3110929, \"EndTime\": 1680916112.7214723, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 409.76786613464355, \"count\": 1, \"min\": 409.76786613464355, \"max\": 409.76786613464355}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1636.88676405242 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] #quality_metric: host=algo-1, epoch=151, train loss <loss>=3.7403332103382456\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] Epoch[152] Batch[0] avg_epoch_loss=3.857452\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:32 INFO 140448693614400] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=3.857452392578125\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] Epoch[152] Batch[5] avg_epoch_loss=3.668645\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=3.6686450242996216\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] Epoch[152] Batch [5]#011Speed: 2427.90 samples/sec#011loss=3.668645\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] Epoch[152] Batch[10] avg_epoch_loss=3.671732\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] #quality_metric: host=algo-1, epoch=152, batch=10 train loss <loss>=3.6754355907440184\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] Epoch[152] Batch [10]#011Speed: 2733.48 samples/sec#011loss=3.675436\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916112.7215822, \"EndTime\": 1680916113.150184, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 427.9448986053467, \"count\": 1, \"min\": 427.9448986053467, \"max\": 427.9448986053467}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1607.1660749124621 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] #progress_metric: host=algo-1, completed 38.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] #quality_metric: host=algo-1, epoch=152, train loss <loss>=3.671731645410711\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] Epoch[153] Batch[0] avg_epoch_loss=3.092246\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=3.0922458171844482\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] Epoch[153] Batch[5] avg_epoch_loss=3.550500\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=3.55049995581309\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] Epoch[153] Batch [5]#011Speed: 2447.43 samples/sec#011loss=3.550500\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] Epoch[153] Batch[10] avg_epoch_loss=3.640450\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] #quality_metric: host=algo-1, epoch=153, batch=10 train loss <loss>=3.748389720916748\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] Epoch[153] Batch [10]#011Speed: 2348.52 samples/sec#011loss=3.748390\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] processed a total of 732 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916113.150275, \"EndTime\": 1680916113.6108541, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 459.90729331970215, \"count\": 1, \"min\": 459.90729331970215, \"max\": 459.90729331970215}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1591.1722628191176 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] #progress_metric: host=algo-1, completed 38.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] #quality_metric: host=algo-1, epoch=153, train loss <loss>=3.6156133015950522\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] Epoch[154] Batch[0] avg_epoch_loss=3.333390\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=3.333390235900879\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] Epoch[154] Batch[5] avg_epoch_loss=3.565060\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=3.565060297648112\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:33 INFO 140448693614400] Epoch[154] Batch [5]#011Speed: 2586.67 samples/sec#011loss=3.565060\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916113.6109395, \"EndTime\": 1680916114.0101202, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 398.4096050262451, \"count\": 1, \"min\": 398.4096050262451, \"max\": 398.4096050262451}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1542.539922499163 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] #progress_metric: host=algo-1, completed 38.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] #quality_metric: host=algo-1, epoch=154, train loss <loss>=3.495692229270935\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] Epoch[155] Batch[0] avg_epoch_loss=3.310680\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=3.310680389404297\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] Epoch[155] Batch[5] avg_epoch_loss=3.459525\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=3.4595245917638144\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] Epoch[155] Batch [5]#011Speed: 2937.84 samples/sec#011loss=3.459525\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] Epoch[155] Batch[10] avg_epoch_loss=3.546144\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] #quality_metric: host=algo-1, epoch=155, batch=10 train loss <loss>=3.650088357925415\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] Epoch[155] Batch [10]#011Speed: 2391.97 samples/sec#011loss=3.650088\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916114.0103543, \"EndTime\": 1680916114.4281135, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 417.03200340270996, \"count\": 1, \"min\": 417.03200340270996, \"max\": 417.03200340270996}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1625.2411722245622 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] #quality_metric: host=algo-1, epoch=155, train loss <loss>=3.546144485473633\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] Epoch[156] Batch[0] avg_epoch_loss=3.557948\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=3.557947874069214\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] Epoch[156] Batch[5] avg_epoch_loss=3.583829\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=3.5838292837142944\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] Epoch[156] Batch [5]#011Speed: 2713.32 samples/sec#011loss=3.583829\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] Epoch[156] Batch[10] avg_epoch_loss=3.615767\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] #quality_metric: host=algo-1, epoch=156, batch=10 train loss <loss>=3.654091501235962\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] Epoch[156] Batch [10]#011Speed: 2745.63 samples/sec#011loss=3.654092\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916114.428204, \"EndTime\": 1680916114.8508701, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 422.0888614654541, \"count\": 1, \"min\": 422.0888614654541, \"max\": 422.0888614654541}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1598.4254875890279 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] #progress_metric: host=algo-1, completed 39.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] #quality_metric: host=algo-1, epoch=156, train loss <loss>=3.6157666553150523\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:34 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] Epoch[157] Batch[0] avg_epoch_loss=3.474633\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=3.474632501602173\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] Epoch[157] Batch[5] avg_epoch_loss=3.602441\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=3.602440516153971\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] Epoch[157] Batch [5]#011Speed: 3002.07 samples/sec#011loss=3.602441\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] Epoch[157] Batch[10] avg_epoch_loss=3.494836\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=157, batch=10 train loss <loss>=3.365710639953613\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] Epoch[157] Batch [10]#011Speed: 2819.58 samples/sec#011loss=3.365711\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916114.851, \"EndTime\": 1680916115.25088, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 399.1434574127197, \"count\": 1, \"min\": 399.1434574127197, \"max\": 399.1434574127197}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1698.0845895977366 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] #progress_metric: host=algo-1, completed 39.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=157, train loss <loss>=3.4948360269719903\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] Epoch[158] Batch[0] avg_epoch_loss=3.623333\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=3.623332977294922\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] Epoch[158] Batch[5] avg_epoch_loss=3.448223\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=3.4482234716415405\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] Epoch[158] Batch [5]#011Speed: 2586.48 samples/sec#011loss=3.448223\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] Epoch[158] Batch[10] avg_epoch_loss=3.398102\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=158, batch=10 train loss <loss>=3.3379558801651\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] Epoch[158] Batch [10]#011Speed: 2816.62 samples/sec#011loss=3.337956\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916115.2509751, \"EndTime\": 1680916115.6630938, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 411.4251136779785, \"count\": 1, \"min\": 411.4251136779785, \"max\": 411.4251136779785}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1601.1844951863895 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] #progress_metric: host=algo-1, completed 39.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=158, train loss <loss>=3.3981018391522495\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] Epoch[159] Batch[0] avg_epoch_loss=3.938829\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=3.9388294219970703\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] Epoch[159] Batch[5] avg_epoch_loss=4.040937\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=4.040937105814616\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:35 INFO 140448693614400] Epoch[159] Batch [5]#011Speed: 3093.41 samples/sec#011loss=4.040937\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] Epoch[159] Batch[10] avg_epoch_loss=3.994355\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] #quality_metric: host=algo-1, epoch=159, batch=10 train loss <loss>=3.938456726074219\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] Epoch[159] Batch [10]#011Speed: 2432.51 samples/sec#011loss=3.938457\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] processed a total of 715 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916115.6631846, \"EndTime\": 1680916116.1508932, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 487.23506927490234, \"count\": 1, \"min\": 487.23506927490234, \"max\": 487.23506927490234}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1466.809565453866 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] #quality_metric: host=algo-1, epoch=159, train loss <loss>=4.060828447341919\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] Epoch[160] Batch[0] avg_epoch_loss=4.416875\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=4.416874885559082\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] Epoch[160] Batch[5] avg_epoch_loss=4.274633\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=4.274632692337036\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] Epoch[160] Batch [5]#011Speed: 3084.89 samples/sec#011loss=4.274633\u001b[0m\n",
      "\n",
      "2023-04-08 01:08:47 Uploading - Uploading generated training model\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] Epoch[160] Batch[10] avg_epoch_loss=4.186718\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] #quality_metric: host=algo-1, epoch=160, batch=10 train loss <loss>=4.081219959259033\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] Epoch[160] Batch [10]#011Speed: 2847.11 samples/sec#011loss=4.081220\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916116.1510644, \"EndTime\": 1680916116.5481744, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 396.46124839782715, \"count\": 1, \"min\": 396.46124839782715, \"max\": 396.46124839782715}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1644.0468802509579 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] #progress_metric: host=algo-1, completed 40.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] #quality_metric: host=algo-1, epoch=160, train loss <loss>=4.186717813665217\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] Epoch[161] Batch[0] avg_epoch_loss=4.736332\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=4.736332416534424\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] Epoch[161] Batch[5] avg_epoch_loss=4.129251\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=4.129250844319661\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:36 INFO 140448693614400] Epoch[161] Batch [5]#011Speed: 2966.94 samples/sec#011loss=4.129251\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] Epoch[161] Batch[10] avg_epoch_loss=4.038383\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] #quality_metric: host=algo-1, epoch=161, batch=10 train loss <loss>=3.929341697692871\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] Epoch[161] Batch [10]#011Speed: 2639.41 samples/sec#011loss=3.929342\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916116.548255, \"EndTime\": 1680916117.0139792, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 464.306116104126, \"count\": 1, \"min\": 464.306116104126, \"max\": 464.306116104126}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1440.4869624142366 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] #progress_metric: host=algo-1, completed 40.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] #quality_metric: host=algo-1, epoch=161, train loss <loss>=4.0383830503983935\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] Epoch[162] Batch[0] avg_epoch_loss=3.925414\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=3.9254138469696045\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] Epoch[162] Batch[5] avg_epoch_loss=3.804302\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=3.804302453994751\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] Epoch[162] Batch [5]#011Speed: 2900.19 samples/sec#011loss=3.804302\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] Epoch[162] Batch[10] avg_epoch_loss=3.849079\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] #quality_metric: host=algo-1, epoch=162, batch=10 train loss <loss>=3.902811336517334\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] Epoch[162] Batch [10]#011Speed: 2697.82 samples/sec#011loss=3.902811\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916117.0140607, \"EndTime\": 1680916117.4558, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 441.1039352416992, \"count\": 1, \"min\": 441.1039352416992, \"max\": 441.1039352416992}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1491.2234804644654 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] #progress_metric: host=algo-1, completed 40.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] #quality_metric: host=algo-1, epoch=162, train loss <loss>=3.849079218777743\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] Epoch[163] Batch[0] avg_epoch_loss=3.556667\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=3.5566670894622803\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] Epoch[163] Batch[5] avg_epoch_loss=3.730219\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=3.730219086011251\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] Epoch[163] Batch [5]#011Speed: 2344.95 samples/sec#011loss=3.730219\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] Epoch[163] Batch[10] avg_epoch_loss=3.754394\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] #quality_metric: host=algo-1, epoch=163, batch=10 train loss <loss>=3.783403921127319\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] Epoch[163] Batch [10]#011Speed: 2552.62 samples/sec#011loss=3.783404\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916117.4559007, \"EndTime\": 1680916117.952547, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 496.00672721862793, \"count\": 1, \"min\": 496.00672721862793, \"max\": 496.00672721862793}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1386.6767028430975 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] #quality_metric: host=algo-1, epoch=163, train loss <loss>=3.7543940110640093\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:37 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] Epoch[164] Batch[0] avg_epoch_loss=3.540191\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=3.5401906967163086\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] Epoch[164] Batch[5] avg_epoch_loss=3.658712\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=3.658711552619934\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] Epoch[164] Batch [5]#011Speed: 2968.31 samples/sec#011loss=3.658712\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] Epoch[164] Batch[10] avg_epoch_loss=3.683255\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] #quality_metric: host=algo-1, epoch=164, batch=10 train loss <loss>=3.7127071380615235\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] Epoch[164] Batch [10]#011Speed: 2677.64 samples/sec#011loss=3.712707\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916117.9526515, \"EndTime\": 1680916118.4133453, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 460.18075942993164, \"count\": 1, \"min\": 460.18075942993164, \"max\": 460.18075942993164}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1489.921935407033 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] #progress_metric: host=algo-1, completed 41.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] #quality_metric: host=algo-1, epoch=164, train loss <loss>=3.683255000547929\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] Epoch[165] Batch[0] avg_epoch_loss=3.220345\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=3.2203450202941895\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] Epoch[165] Batch[5] avg_epoch_loss=3.389174\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=3.3891737858454385\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] Epoch[165] Batch [5]#011Speed: 3102.86 samples/sec#011loss=3.389174\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] Epoch[165] Batch[10] avg_epoch_loss=3.479898\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] #quality_metric: host=algo-1, epoch=165, batch=10 train loss <loss>=3.5887669563293456\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] Epoch[165] Batch [10]#011Speed: 2443.00 samples/sec#011loss=3.588767\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] processed a total of 707 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916118.413544, \"EndTime\": 1680916118.8454468, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 431.11610412597656, \"count\": 1, \"min\": 431.11610412597656, \"max\": 431.11610412597656}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1639.3936432696842 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] #progress_metric: host=algo-1, completed 41.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] #quality_metric: host=algo-1, epoch=165, train loss <loss>=3.4540680845578513\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:38 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] Epoch[166] Batch[0] avg_epoch_loss=3.660244\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=3.6602439880371094\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] Epoch[166] Batch[5] avg_epoch_loss=3.503745\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=3.5037450393040976\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] Epoch[166] Batch [5]#011Speed: 3118.97 samples/sec#011loss=3.503745\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916118.8455403, \"EndTime\": 1680916119.2713702, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 425.28843879699707, \"count\": 1, \"min\": 425.28843879699707, \"max\": 425.28843879699707}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1499.664841939143 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] #progress_metric: host=algo-1, completed 41.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] #quality_metric: host=algo-1, epoch=166, train loss <loss>=3.552891707420349\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] Epoch[167] Batch[0] avg_epoch_loss=3.624230\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=3.624229907989502\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] Epoch[167] Batch[5] avg_epoch_loss=3.513795\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=3.5137954552968345\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] Epoch[167] Batch [5]#011Speed: 2704.41 samples/sec#011loss=3.513795\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] Epoch[167] Batch[10] avg_epoch_loss=3.405674\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] #quality_metric: host=algo-1, epoch=167, batch=10 train loss <loss>=3.2759271621704102\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] Epoch[167] Batch [10]#011Speed: 2508.50 samples/sec#011loss=3.275927\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916119.2714624, \"EndTime\": 1680916119.7021313, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 429.81719970703125, \"count\": 1, \"min\": 429.81719970703125, \"max\": 429.81719970703125}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1497.7588919897375 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] #quality_metric: host=algo-1, epoch=167, train loss <loss>=3.4056735038757324\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] Epoch[168] Batch[0] avg_epoch_loss=3.328563\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=3.3285632133483887\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] Epoch[168] Batch[5] avg_epoch_loss=3.445475\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=3.4454749822616577\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:39 INFO 140448693614400] Epoch[168] Batch [5]#011Speed: 3071.01 samples/sec#011loss=3.445475\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] Epoch[168] Batch[10] avg_epoch_loss=3.481342\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] #quality_metric: host=algo-1, epoch=168, batch=10 train loss <loss>=3.524381399154663\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] Epoch[168] Batch [10]#011Speed: 2744.24 samples/sec#011loss=3.524381\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] processed a total of 691 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916119.7022414, \"EndTime\": 1680916120.1049664, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 402.1177291870117, \"count\": 1, \"min\": 402.1177291870117, \"max\": 402.1177291870117}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1717.7789115323706 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] #progress_metric: host=algo-1, completed 42.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] #quality_metric: host=algo-1, epoch=168, train loss <loss>=3.481341535394842\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] Epoch[169] Batch[0] avg_epoch_loss=3.958302\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=3.958301544189453\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] Epoch[169] Batch[5] avg_epoch_loss=3.738858\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=3.738858461380005\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] Epoch[169] Batch [5]#011Speed: 3068.47 samples/sec#011loss=3.738858\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] Epoch[169] Batch[10] avg_epoch_loss=3.799667\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] #quality_metric: host=algo-1, epoch=169, batch=10 train loss <loss>=3.8726365089416506\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] Epoch[169] Batch [10]#011Speed: 2626.07 samples/sec#011loss=3.872637\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] processed a total of 696 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916120.105065, \"EndTime\": 1680916120.5054767, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 399.8074531555176, \"count\": 1, \"min\": 399.8074531555176, \"max\": 399.8074531555176}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1740.201847123772 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] #quality_metric: host=algo-1, epoch=169, train loss <loss>=3.7996666648171167\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] Epoch[170] Batch[0] avg_epoch_loss=3.787421\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=3.787421226501465\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] Epoch[170] Batch[5] avg_epoch_loss=3.590137\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=3.5901371240615845\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] Epoch[170] Batch [5]#011Speed: 2842.70 samples/sec#011loss=3.590137\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] Epoch[170] Batch[10] avg_epoch_loss=3.580791\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] #quality_metric: host=algo-1, epoch=170, batch=10 train loss <loss>=3.56957631111145\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] Epoch[170] Batch [10]#011Speed: 2575.71 samples/sec#011loss=3.569576\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916120.5055752, \"EndTime\": 1680916120.9180813, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 411.93413734436035, \"count\": 1, \"min\": 411.93413734436035, \"max\": 411.93413734436035}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1649.992982634769 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] #progress_metric: host=algo-1, completed 42.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] #quality_metric: host=algo-1, epoch=170, train loss <loss>=3.580791299993342\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:40 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] Epoch[171] Batch[0] avg_epoch_loss=4.003927\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=4.003927230834961\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] Epoch[171] Batch[5] avg_epoch_loss=3.567341\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=3.567340890566508\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] Epoch[171] Batch [5]#011Speed: 2927.07 samples/sec#011loss=3.567341\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] Epoch[171] Batch[10] avg_epoch_loss=3.745448\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] #quality_metric: host=algo-1, epoch=171, batch=10 train loss <loss>=3.959177017211914\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] Epoch[171] Batch [10]#011Speed: 2616.66 samples/sec#011loss=3.959177\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916120.9182308, \"EndTime\": 1680916121.386058, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 467.332124710083, \"count\": 1, \"min\": 467.332124710083, \"max\": 467.332124710083}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1420.4114913358005 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] #quality_metric: host=algo-1, epoch=171, train loss <loss>=3.7454482208598745\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] Epoch[172] Batch[0] avg_epoch_loss=3.270411\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=3.2704105377197266\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] Epoch[172] Batch[5] avg_epoch_loss=3.482173\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=3.482173045476278\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] Epoch[172] Batch [5]#011Speed: 2944.82 samples/sec#011loss=3.482173\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] Epoch[172] Batch[10] avg_epoch_loss=3.521509\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] #quality_metric: host=algo-1, epoch=172, batch=10 train loss <loss>=3.5687116622924804\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] Epoch[172] Batch [10]#011Speed: 2582.49 samples/sec#011loss=3.568712\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] processed a total of 689 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916121.3861494, \"EndTime\": 1680916121.7955115, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 408.7836742401123, \"count\": 1, \"min\": 408.7836742401123, \"max\": 408.7836742401123}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1684.894506482459 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] #progress_metric: host=algo-1, completed 43.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] #quality_metric: host=algo-1, epoch=172, train loss <loss>=3.5215087803927334\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:41 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] Epoch[173] Batch[0] avg_epoch_loss=3.519418\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=3.5194175243377686\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] Epoch[173] Batch[5] avg_epoch_loss=3.638170\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=3.638169606526693\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] Epoch[173] Batch [5]#011Speed: 2135.39 samples/sec#011loss=3.638170\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] Epoch[173] Batch[10] avg_epoch_loss=3.738214\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] #quality_metric: host=algo-1, epoch=173, batch=10 train loss <loss>=3.8582675457000732\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] Epoch[173] Batch [10]#011Speed: 2376.54 samples/sec#011loss=3.858268\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916121.7956119, \"EndTime\": 1680916122.3189654, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 522.5880146026611, \"count\": 1, \"min\": 522.5880146026611, \"max\": 522.5880146026611}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1249.2014896203202 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] #progress_metric: host=algo-1, completed 43.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] #quality_metric: host=algo-1, epoch=173, train loss <loss>=3.738214124332775\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] Epoch[174] Batch[0] avg_epoch_loss=3.635532\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=3.6355316638946533\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] Epoch[174] Batch[5] avg_epoch_loss=3.636477\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=3.636476755142212\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] Epoch[174] Batch [5]#011Speed: 2168.71 samples/sec#011loss=3.636477\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] Epoch[174] Batch[10] avg_epoch_loss=3.661277\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] #quality_metric: host=algo-1, epoch=174, batch=10 train loss <loss>=3.6910378456115724\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] Epoch[174] Batch [10]#011Speed: 1936.17 samples/sec#011loss=3.691038\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916122.3190625, \"EndTime\": 1680916122.8687508, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 549.0691661834717, \"count\": 1, \"min\": 549.0691661834717, \"max\": 549.0691661834717}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1239.703834360155 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] #progress_metric: host=algo-1, completed 43.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] #quality_metric: host=algo-1, epoch=174, train loss <loss>=3.661277250810103\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:42 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] Epoch[175] Batch[0] avg_epoch_loss=3.897916\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=3.897916078567505\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] Epoch[175] Batch[5] avg_epoch_loss=3.875306\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=3.875306487083435\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] Epoch[175] Batch [5]#011Speed: 2656.07 samples/sec#011loss=3.875306\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] Epoch[175] Batch[10] avg_epoch_loss=3.658697\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] #quality_metric: host=algo-1, epoch=175, batch=10 train loss <loss>=3.3987667083740236\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] Epoch[175] Batch [10]#011Speed: 2040.00 samples/sec#011loss=3.398767\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916122.8689332, \"EndTime\": 1680916123.3843102, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 514.1627788543701, \"count\": 1, \"min\": 514.1627788543701, \"max\": 514.1627788543701}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1310.2722669414245 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] #quality_metric: host=algo-1, epoch=175, train loss <loss>=3.658697496760975\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] Epoch[176] Batch[0] avg_epoch_loss=3.641861\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=3.6418609619140625\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] Epoch[176] Batch[5] avg_epoch_loss=3.635492\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=3.635491728782654\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] Epoch[176] Batch [5]#011Speed: 2966.01 samples/sec#011loss=3.635492\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] Epoch[176] Batch[10] avg_epoch_loss=3.606329\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] #quality_metric: host=algo-1, epoch=176, batch=10 train loss <loss>=3.5713340282440185\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] Epoch[176] Batch [10]#011Speed: 2814.37 samples/sec#011loss=3.571334\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916123.3844998, \"EndTime\": 1680916123.8357675, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 450.20079612731934, \"count\": 1, \"min\": 450.20079612731934, \"max\": 450.20079612731934}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1474.314980479348 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] #progress_metric: host=algo-1, completed 44.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] #quality_metric: host=algo-1, epoch=176, train loss <loss>=3.6063291376287285\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:43 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] Epoch[177] Batch[0] avg_epoch_loss=3.575314\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=3.5753140449523926\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] Epoch[177] Batch[5] avg_epoch_loss=3.808368\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=3.808367927869161\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] Epoch[177] Batch [5]#011Speed: 3018.41 samples/sec#011loss=3.808368\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] Epoch[177] Batch[10] avg_epoch_loss=3.811233\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] #quality_metric: host=algo-1, epoch=177, batch=10 train loss <loss>=3.8146699905395507\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] Epoch[177] Batch [10]#011Speed: 2736.91 samples/sec#011loss=3.814670\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916123.8359027, \"EndTime\": 1680916124.2352664, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 398.65899085998535, \"count\": 1, \"min\": 398.65899085998535, \"max\": 398.65899085998535}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1692.6338294174514 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] #progress_metric: host=algo-1, completed 44.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] #quality_metric: host=algo-1, epoch=177, train loss <loss>=3.811232501810247\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] Epoch[178] Batch[0] avg_epoch_loss=3.609265\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=3.6092653274536133\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] Epoch[178] Batch[5] avg_epoch_loss=3.795676\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=3.795676112174988\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] Epoch[178] Batch [5]#011Speed: 2379.36 samples/sec#011loss=3.795676\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] Epoch[178] Batch[10] avg_epoch_loss=3.821869\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] #quality_metric: host=algo-1, epoch=178, batch=10 train loss <loss>=3.853300857543945\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] Epoch[178] Batch [10]#011Speed: 2404.96 samples/sec#011loss=3.853301\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] processed a total of 704 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916124.2353544, \"EndTime\": 1680916124.7164814, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 480.6220531463623, \"count\": 1, \"min\": 480.6220531463623, \"max\": 480.6220531463623}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1464.2301836343656 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] #progress_metric: host=algo-1, completed 44.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] #quality_metric: host=algo-1, epoch=178, train loss <loss>=3.8218691782517866\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] Epoch[179] Batch[0] avg_epoch_loss=3.729387\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:44 INFO 140448693614400] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=3.729386806488037\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] Epoch[179] Batch[5] avg_epoch_loss=3.612695\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=3.6126948595046997\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] Epoch[179] Batch [5]#011Speed: 2324.39 samples/sec#011loss=3.612695\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] Epoch[179] Batch[10] avg_epoch_loss=3.597944\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] #quality_metric: host=algo-1, epoch=179, batch=10 train loss <loss>=3.5802433490753174\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] Epoch[179] Batch [10]#011Speed: 2217.47 samples/sec#011loss=3.580243\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916124.7166157, \"EndTime\": 1680916125.235191, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 518.0737972259521, \"count\": 1, \"min\": 518.0737972259521, \"max\": 518.0737972259521}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1238.8140331958377 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] #quality_metric: host=algo-1, epoch=179, train loss <loss>=3.5979441729458896\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] Epoch[180] Batch[0] avg_epoch_loss=3.697697\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=3.6976969242095947\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] Epoch[180] Batch[5] avg_epoch_loss=3.646077\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=3.646076520284017\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] Epoch[180] Batch [5]#011Speed: 2287.20 samples/sec#011loss=3.646077\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] Epoch[180] Batch[10] avg_epoch_loss=3.620363\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] #quality_metric: host=algo-1, epoch=180, batch=10 train loss <loss>=3.5895057201385496\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] Epoch[180] Batch [10]#011Speed: 2245.85 samples/sec#011loss=3.589506\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] processed a total of 687 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916125.2353003, \"EndTime\": 1680916125.799602, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 562.7119541168213, \"count\": 1, \"min\": 562.7119541168213, \"max\": 562.7119541168213}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] #throughput_metric: host=algo-1, train throughput=1220.5438817496618 records/second\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] #progress_metric: host=algo-1, completed 45.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] #quality_metric: host=algo-1, epoch=180, train loss <loss>=3.6203625202178955\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] Loading parameters from best epoch (140)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916125.7997072, \"EndTime\": 1680916125.8080304, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 7.347822189331055, \"count\": 1, \"min\": 7.347822189331055, \"max\": 7.347822189331055}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] stopping training now\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] Final loss: 3.390558938185374 (occurred at epoch 140)\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] #quality_metric: host=algo-1, train final_loss <loss>=3.390558938185374\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 WARNING 140448693614400] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916125.8081229, \"EndTime\": 1680916125.872049, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 63.05646896362305, \"count\": 1, \"min\": 63.05646896362305, \"max\": 63.05646896362305}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916125.8721414, \"EndTime\": 1680916125.8985374, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 89.59436416625977, \"count\": 1, \"min\": 89.59436416625977, \"max\": 89.59436416625977}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916125.8986273, \"EndTime\": 1680916125.901957, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 3.2846927642822266, \"count\": 1, \"min\": 3.2846927642822266, \"max\": 3.2846927642822266}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] #memory_usage::<batchbuffer> = 0.32958984375 mb\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:45 INFO 140448693614400] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916125.9020197, \"EndTime\": 1680916125.9047282, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.04482269287109375, \"count\": 1, \"min\": 0.04482269287109375, \"max\": 0.04482269287109375}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916125.9048057, \"EndTime\": 1680916126.070761, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 166.07236862182617, \"count\": 1, \"min\": 166.07236862182617, \"max\": 166.07236862182617}}}\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:46 INFO 140448693614400] #test_score (algo-1, RMSE): 15.239822193219874\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:46 INFO 140448693614400] #test_score (algo-1, mean_absolute_QuantileLoss): 109.13017578125\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:46 INFO 140448693614400] #test_score (algo-1, mean_wQuantileLoss): 0.0023449626864262257\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:46 INFO 140448693614400] #test_score (algo-1, wQuantileLoss[0.1]): 0.0016626487638958006\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:46 INFO 140448693614400] #test_score (algo-1, wQuantileLoss[0.2]): 0.0024584092294616285\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:46 INFO 140448693614400] #test_score (algo-1, wQuantileLoss[0.3]): 0.0028086030095785873\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:46 INFO 140448693614400] #test_score (algo-1, wQuantileLoss[0.4]): 0.003144183436280113\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:46 INFO 140448693614400] #test_score (algo-1, wQuantileLoss[0.5]): 0.0033224709644586623\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:46 INFO 140448693614400] #test_score (algo-1, wQuantileLoss[0.6]): 0.003192157371631007\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:46 INFO 140448693614400] #test_score (algo-1, wQuantileLoss[0.7]): 0.002371929401638995\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:46 INFO 140448693614400] #test_score (algo-1, wQuantileLoss[0.8]): 0.0013929270530103801\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:46 INFO 140448693614400] #test_score (algo-1, wQuantileLoss[0.9]): 0.000751334947880858\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:46 INFO 140448693614400] #quality_metric: host=algo-1, test RMSE <loss>=15.239822193219874\u001b[0m\n",
      "\u001b[34m[04/08/2023 01:08:46 INFO 140448693614400] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.0023449626864262257\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1680916126.0708892, \"EndTime\": 1680916126.0788743, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 7.00068473815918, \"count\": 1, \"min\": 7.00068473815918, \"max\": 7.00068473815918}, \"totaltime\": {\"sum\": 81244.58503723145, \"count\": 1, \"min\": 81244.58503723145, \"max\": 81244.58503723145}}}\u001b[0m\n",
      "\n",
      "2023-04-08 01:09:03 Completed - Training job completed\n",
      "Training seconds: 244\n",
      "Billable seconds: 244\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c60f9-b365-4d1e-a84c-3192fc8b9e4a",
   "metadata": {},
   "source": [
    "#### Crear endpoint y predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "57ba17d0-4420-4d41-b5b5-dea2fd236772",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6b98cb52-2591-4255-9d4e-e9b3787534e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Esta clase permiti recibir \n",
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            # serializer=JSONSerializer(),\n",
    "            serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        ts,\n",
    "        cat=None,\n",
    "        dynamic_feat=None,\n",
    "        num_samples=100,\n",
    "        return_samples=False,\n",
    "        quantiles=[\"0.1\", \"0.5\", \"0.9\"],\n",
    "    ):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "\n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "\n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + ts.index.freq\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "\n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(\n",
    "            ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None\n",
    "        )\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles,\n",
    "        }\n",
    "\n",
    "        http_request_data = {\"instances\": [instance], \"configuration\": configuration}\n",
    "\n",
    "        return json.dumps(http_request_data).encode(\"utf-8\")\n",
    "\n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode(\"utf-8\"))[\"predictions\"][0]\n",
    "        prediction_length = len(next(iter(predictions[\"quantiles\"].values())))\n",
    "        prediction_index = pd.date_range(\n",
    "            start=prediction_time, freq=freq, periods=prediction_length\n",
    "        )\n",
    "        if return_samples:\n",
    "            dict_of_samples = {\"sample_\" + str(i): s for i, s in enumerate(predictions[\"samples\"])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(\n",
    "            data={**predictions[\"quantiles\"], **dict_of_samples}, index=prediction_index\n",
    "        )\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "\n",
    "\n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]\n",
    "\n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5011fdfc-489a-4e73-b846-b998cb89c753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: dollar-predictor-2023-04-08-01-11-50-254\n",
      "INFO:sagemaker:Creating endpoint-config with name dollar-predictor-2023-04-08-01-11-50-254\n",
      "INFO:sagemaker:Creating endpoint with name dollar-predictor-2023-04-08-01-11-50-254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m4.xlarge\", predictor_cls=DeepARPredictor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5d984b-47f4-45b5-93fe-be6adffd1bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(ts=df.iloc[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202f11bd-c639-4450-831e-5e62b35c1f9e",
   "metadata": {},
   "source": [
    "### Eliminar Endpoint y Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573262f8-f4be-4ade-a87c-ac615925d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
